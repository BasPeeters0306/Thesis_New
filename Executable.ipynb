{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import importlib\n",
    "from warnings import simplefilter\n",
    "from six import StringIO\n",
    "from IPython.display import Image\n",
    "\n",
    "# Reload external files\n",
    "import importlib\n",
    "\n",
    "# Mute warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change function of EOD because we use Compustat....\n",
    "# In the end, add all used Excel files to this repo, we have: stock_universe_new.joblib, SP500H_EOD_raw, SP500H_RP_SESI (if we only use daily SESI score and no other sentiment feature types)\n",
    "# We use EOD for the list of historical components\n",
    "# At the end, just do all data steps one more time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose data location, EOD, SQL or CSV, Compustat\n",
    "str_load_data = \"CSV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads raw data, data cleaning and transformation is needed after this\n",
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "\n",
    "if str_load_data == \"EOD_old_method\":\n",
    "    \n",
    "    # Index name \n",
    "    str_index_name = \"S&P500_historical\"\n",
    "\n",
    "    # Loads market data from EOD\n",
    "    ar_index, df_index = Load_data.create_stock_universe_old(str_index_name)\n",
    "    df = Load_data.load_EOD(ar_index)\n",
    "\n",
    "    df = Load_data.create_historical_SP_Index_old_method(df_index, df)            \n",
    "\n",
    "    # Adds lags and returns\n",
    "    df = Load_data.add_lags(df)\n",
    "\n",
    "if str_load_data == \"EOD_new_method\":\n",
    "\n",
    "    df = Load_data.create_historical_SP_Index()\n",
    "\n",
    "    # Adds lags and returns\n",
    "    df = Load_data.add_lags(df)\n",
    "\n",
    "elif str_load_data == \"SQL\":\n",
    "    \n",
    "    # Name of the dataframe\n",
    "    str_table_name = \"SP500H_EOD_raw\"\n",
    "\n",
    "    # Specify column subset, None if all columns should be loaded\n",
    "    str_column_subset = None\n",
    "\n",
    "    # Loads the dataframe from SQL\n",
    "    df = Load_data.load_SQL(str_table_name, str_column_subset)\n",
    "\n",
    "elif str_load_data == \"CSV\":\n",
    "\n",
    "    df = Load_data.load_csv(\"df_Compustat_SPH\")\n",
    "\n",
    "# Check the number of unique elements of SymbolExchangeCode for every BarDate\n",
    "# df_test = df.groupby(\"BarDate\")[\"SymbolExchangeCode\"].nunique()\n",
    "\n",
    "# Plot the number of unique elements of SymbolExchangeCode for every BarDate\n",
    "# df.groupby(\"BarDate\")[\"SymbolExchangeCode\"].nunique().plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Still code here -> Create daily SESI -> Add Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SESI = Load_data.load_csv(\"df_SESI\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df with average dayly SESI and market data\n",
    "# Takes long to run, +-1100min\n",
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "\n",
    "df = Load_data.add_SESI(df, df_SESI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop TimeStamp_TZ column from df_SESI\n",
    "df = df.drop(columns = [\"TIMESTAMP_UTC\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load or Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "file_name = \"........\"\n",
    "df = Load_data.load_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Save_data\n",
    "importlib.reload(Save_data)\n",
    "file_name = \"df_Compustat_SPH_SESI\"\n",
    "Save_data.save_to_csv(df, file_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Analyze_data\n",
    "importlib.reload(Analyze_data)\n",
    "\n",
    "# Check the number of unique elements of SymbolExchangeCode for every BarDate\n",
    "Analyze_data.unique_stocks_by_date(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Analyze_data\n",
    "importlib.reload(Analyze_data)\n",
    "\n",
    "# Plot all stocks\n",
    "Analyze_data.plot_all_stocks(df, max_AdjustedClose = 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show summary statistics for CS_MedianNextDayReturn\n",
    "print(df['CS_MedianNextdayReturn'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which metrics you want to check\n",
    "\n",
    "bool_dict = {\n",
    "    'descriptive_stats': True,\n",
    "    'correlation_matrix': False,\n",
    "    \"histogram\": False,\n",
    "    \"boxplot\": False,\n",
    "    \"scatterplot\": False,\n",
    "    \"lineplot\": False,\n",
    "    \"heatmap\": False,\n",
    "    \"barplot\": False,\n",
    "    \"piechart\": False,\n",
    "    \"violinplot\": False,\n",
    "    \"kdeplot\": False,\n",
    "    \"hexbinplot\": False,\n",
    "    \"scatter_matrix\": False,\n",
    "    \"parallel_coordinates\": False,\n",
    "    \"andrews_curves\": False,\n",
    "    \"radviz\": False,\n",
    "    \"lag_plot\": False,\n",
    "    \"autocorrelation_plot\": False,\n",
    "    \"bootstrap_plot\": False\n",
    "}\n",
    "\n",
    "import Analyze_data\n",
    "importlib.reload(Analyze_data)\n",
    "\n",
    "Analyze_data.Data_analysis(df, bool_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import importlib\n",
    "from warnings import simplefilter\n",
    "from six import StringIO\n",
    "from IPython.display import Image\n",
    "\n",
    "# Reload external files\n",
    "import importlib\n",
    "\n",
    "# Mute warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads prepared data, data preparation not needed anymore after this\n",
    "\n",
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "file_name = \"df_Compustat_SPH_SESI\"\n",
    "df = Load_data.load_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Order df by BarDate and SymbolExchangeCode\n",
    "# df = df.sort_values(by = [\"BarDate\", \"Ticker\"])\n",
    "# # reset index\n",
    "# df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 'Hyperparameter' is the sequence_length/time_steps\n",
    "time_steps = 3\n",
    "\n",
    "b_sentiment_score = True\n",
    "\n",
    "n_past_returns = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_split_number = 0.60 # With the larger SP500H index we might be able to get back to 2008 with a reasonable percentage (0.40 min)\n",
    "b_MinMaxScaler = True \n",
    "b_standardizer = False \n",
    "\n",
    "import Data_preparation\n",
    "importlib.reload(Data_preparation)\n",
    "\n",
    "# Scale features with MinMaxScaler or standardize features with StandardScaler\n",
    "df = Data_preparation.feature_scaling(df, b_MinMaxScaler, b_standardizer, percentage_split_number)\n",
    "\n",
    "\n",
    "# Creates splits and features and target dataframes\n",
    "X_in_sample, X_test, y_in_sample, y_test, X_train, X_val, y_train, y_val = Data_preparation.create_splits(df, percentage_split_number, model = \"not_lstm\", b_sentiment_score = b_sentiment_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dataframe which will be filled with all predictions\n",
    "df_classifications = df.loc[:, df.columns.isin([\"BarDate\", \"Ticker\", \"NextdayReturn\", 'Target'])]\n",
    "df_classifications = df_classifications.tail(len(X_test))\n",
    "\n",
    "# Creates dataframe which will be filled with all classifications\n",
    "df_predictions = df.loc[:, df.columns.isin([\"BarDate\", \"Ticker\", \"NextdayReturn\", 'Target'])]\n",
    "df_predictions = df_predictions.tail(len(X_test))\n",
    "\n",
    "# Creates dataframe which will be filled with all Accuracy values\n",
    "df_accuracy = pd.DataFrame({}, index=[\"All\", \"Top 10\", \"Top 20\", \"Top decile\", \"Top quintile\", \"Second quintile\", \"Third quintile\", \"Fourth quintile\", \"Bottom quintile\", \"Bottom decile\", \"Bottom 20\", \"Bottom 10\"])\n",
    "# Expanding window size for predictions in the test set +- 1 year\n",
    "window_size = 500*500\n",
    "\n",
    "# Change dtypes to 32 for memory efficiency\n",
    "df['PreviousdayReturn'] = df['PreviousdayReturn'].astype('float32')\n",
    "df['PreviousdayReturn_2'] = df['PreviousdayReturn_2'].astype('float32')\n",
    "df['PreviousdayReturn_3'] = df['PreviousdayReturn_3'].astype('float32')\n",
    "df[\"Target\"] = df[\"Target\"].astype('float32')\n",
    "df[\"SESI\"] = df[\"SESI\"].astype('float32')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Logistic_regression\n",
    "importlib.reload(Logistic_regression)\n",
    "\n",
    "ar_classifications_lr, ar_predictions_lr, model_lr = Logistic_regression.logistic_regression(window_size, y_in_sample, X_in_sample, y_test, X_test)\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_predictions[\"predictions_lr\"] = ar_predictions_lr\n",
    "# Add classifications to df_classifications\n",
    "df_classifications[\"classifications_lr\"] = ar_classifications_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Evaluation_metrics\n",
    "importlib.reload(Evaluation_metrics)\n",
    "\n",
    "predictions_accuracy = Evaluation_metrics.prediction_metrics(df_classifications, str_model = \"lr\", str_single_metric = \"Accuracy\")\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_accuracy[\"Logistic Regresion\"] = predictions_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with ENet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ENet\n",
    "# importlib.reload(ENet)\n",
    "\n",
    "# # Specify the grid\n",
    "# grid = {\"l1_ratio\": [0.3, 0.5, 0.7], \"C\": np.logspace(start=-40, stop=-10, num=10, base=10)}\n",
    "# # Predictions \n",
    "# performances_ENet, best_model_ENet = ENet.ENet_tune(X_train, y_train, X_val, y_val, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # At the moment this returns 50% proba always, not sure if ENEt a good model for our features\n",
    "# # Or something is wrong with the code\n",
    "\n",
    "# import ENet\n",
    "# importlib.reload(ENet)\n",
    "\n",
    "# ar_classifications_enet, ar_predictions_enet, model_enet = ENet.ENet_test(best_model_ENet, window_size, y_in_sample, X_in_sample, y_test, X_test)\n",
    "\n",
    "# # Add predictions to df_predictions\n",
    "# df_predictions[\"predictions_enet\"] = ar_predictions_enet\n",
    "# # Add classifications to df_classifications\n",
    "# df_classifications[\"classifications_enet\"] = ar_classifications_enet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Evaluation_metrics\n",
    "# importlib.reload(Evaluation_metrics)\n",
    "\n",
    "# predictions_accuracy = Evaluation_metrics.prediction_metrics(df_classifications, str_model = \"enet\", str_single_metric = \"Accuracy\")\n",
    "\n",
    "# # Add predictions to df_predictions\n",
    "# df_accuracy[\"Elastic Net\"] = predictions_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "\n",
    "# Should be 1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 234min for PC1, (with tuning 4*4 combinations), with SESI\n",
    "# 120min for PC3, Without SESI\n",
    "# 160min for PC2, Without SESI\n",
    "\n",
    "import RF\n",
    "importlib.reload(RF)\n",
    "\n",
    "# Specify the grid \n",
    "# n_jobs = -1 makes use of all cores\n",
    "# When setting max_depth > 2 and more then 100000 rows, the kernel crashes\n",
    "# Try a different RF package?\n",
    "grid = {\"n_estimators\": [500, 700, 900],   \n",
    "        \"max_depth\": [2, 4, 6, 8]}\n",
    "                                                  #\"max_features\":    [3, 5, 10]} # Not needed because we only have 4 features\n",
    "# Predictions \n",
    "performances_RF, best_model_RF =  RF.RF_tune(X_train, y_train, X_val, y_val, grid)   #RF.RF_tune(X_train_1000, y_train_1000, X_val_1000, y_val_1000, grid)                       #RF.RF_tune(X_train, y_train, X_val, y_val, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 213min for PC1, with SESI\n",
    "# 120min for PC3, Without SESI\n",
    "# 220min for PC2, Without SESI\n",
    "\n",
    "import RF\n",
    "importlib.reload(RF)\n",
    "\n",
    "ar_classifications_rf, ar_predictions_rf, model_rf = RF.RF_test(best_model_RF, window_size, y_in_sample, X_in_sample, y_test, X_test)\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_predictions[\"predictions_rf\"] = ar_predictions_rf\n",
    "# Add classifications to df_classifications\n",
    "df_classifications[\"classifications_rf\"] = ar_classifications_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Evaluation_metrics\n",
    "importlib.reload(Evaluation_metrics)\n",
    "\n",
    "predictions_accuracy = Evaluation_metrics.prediction_metrics(df_classifications, str_model = \"rf\", str_single_metric = \"Accuracy\")\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_accuracy[\"Random Forest\"] = predictions_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 210min for PC1, with SESI\n",
    "# 160min for PC3, without SESI\n",
    "# 180min for PC2, Without SESI\n",
    "\n",
    "import GBC\n",
    "importlib.reload(GBC)\n",
    "\n",
    "# Specify the grid \n",
    "grid = {\"n_estimators\": [100, 200, 500],\n",
    "        \"learning_rate\":[0.01, 0.1, 0.2],\n",
    "        \"max_depth\": [1, 2, 3]}\n",
    "# Predictions \n",
    "performances_GBC, best_model_GBC = GBC.GBC_tune(X_train, y_train, X_val, y_val, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... min for PC1, with SESI\n",
    "# 35 min for PC3, Without SESI\n",
    "# 40min for PC2, Without SESI\n",
    "\n",
    "import GBC\n",
    "importlib.reload(GBC)\n",
    "\n",
    "ar_classifications_gbc, ar_predictions_gbc, model_gbc = GBC.GBC_test(best_model_GBC, window_size, y_in_sample, X_in_sample, y_test, X_test)\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_predictions[\"predictions_gbc\"] = ar_predictions_gbc\n",
    "# Add classifications to df_classifications\n",
    "df_classifications[\"classifications_gbc\"] = ar_classifications_gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Evaluation_metrics\n",
    "importlib.reload(Evaluation_metrics)\n",
    "\n",
    "predictions_accuracy = Evaluation_metrics.prediction_metrics(df_classifications, str_model = \"gbc\", str_single_metric = \"Accuracy\")\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_accuracy[\"Gradient Boosting Classifier\"] = predictions_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 'Hyperparameter' is the sequence_length/time_steps\n",
    "model = \"lstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_data(stock_df, time_steps):\n",
    "    \n",
    "    seq = []\n",
    "    if len(stock_df) <= time_steps:\n",
    "        return seq\n",
    "    for i in range(len(stock_df) - time_steps):\n",
    "        v = stock_df.iloc[i:(i + time_steps)]\n",
    "        seq.append(v)\n",
    "    # print(\"seq length: \", len(seq))\n",
    "    return pd.concat(seq, keys=range(len(seq)))\n",
    "\n",
    "if b_sentiment_score == True and n_past_returns == 1:\n",
    "    X_columns = [\"BarDate\", \"Ticker\", \"PreviousdayReturn\", \"SESI\"]\n",
    "elif b_sentiment_score == False and n_past_returns == 1:\n",
    "    X_columns = [\"BarDate\", \"Ticker\", \"PreviousdayReturn\"]\n",
    "elif b_sentiment_score == True and n_past_returns == 3:\n",
    "    X_columns = [\"BarDate\", \"Ticker\", \"PreviousdayReturn\", \"PreviousdayReturn_2\", \"PreviousdayReturn_3\", \"SESI\"]\n",
    "elif b_sentiment_score == False and n_past_returns == 3:\n",
    "    X_columns = [\"BarDate\", \"Ticker\", \"PreviousdayReturn\", \"PreviousdayReturn_2\", \"PreviousdayReturn_3\"]\n",
    "\n",
    "# if b_sentiment_score == True:\n",
    "X_in_sample_lstm_grouped = X_in_sample.groupby(\"Ticker\")[X_columns]\n",
    "X_test_lstm_grouped = X_test.groupby(\"Ticker\")[X_columns]\n",
    "X_train_lstm_grouped = X_train.groupby(\"Ticker\")[X_columns]\n",
    "X_val_lstm_grouped = X_val.groupby(\"Ticker\")[X_columns]\n",
    "# elif b_sentiment_score == False:\n",
    "#     X_in_sample_lstm_grouped = X_in_sample.groupby(\"Ticker\")[[\"BarDate\", \"Ticker\", \"PreviousdayReturn\"]]\n",
    "#     X_test_lstm_grouped = X_test.groupby(\"Ticker\")[[\"BarDate\", \"Ticker\", \"PreviousdayReturn\"]]\n",
    "#     X_train_lstm_grouped = X_train.groupby(\"Ticker\")[[\"BarDate\", \"Ticker\", \"PreviousdayReturn\"]]\n",
    "#     X_val_lstm_grouped = X_val.groupby(\"Ticker\")[[\"BarDate\", \"Ticker\", \"PreviousdayReturn\"]]\n",
    "\n",
    "y_in_sample_lstm_grouped = y_in_sample.groupby(\"Ticker\")[[\"BarDate\", \"Ticker\", \"Target\"]]\n",
    "y_test_lstm_grouped = y_test.groupby(\"Ticker\")[[\"BarDate\", \"Ticker\", \"Target\"]]\n",
    "y_train_lstm_grouped = y_train.groupby(\"Ticker\")[[\"BarDate\", \"Ticker\", \"Target\"]]\n",
    "y_val_lstm_grouped = y_val.groupby(\"Ticker\")[[\"BarDate\", \"Ticker\", \"Target\"]]\n",
    "\n",
    "\n",
    "X_in_sample_lstm_seq = [create_lstm_data(group, time_steps) for _, group in X_in_sample_lstm_grouped]\n",
    "# print(X_in_sample_lstm_seq)\n",
    "X_in_sample_lstm_seq = [seq for seq in X_in_sample_lstm_seq if len(seq) > 0]\n",
    "X_in_sample_lstm = pd.concat(X_in_sample_lstm_seq) if X_in_sample_lstm_seq else pd.DataFrame()\n",
    "print(\"first is done\")\n",
    "\n",
    "X_test_lstm_seq = [create_lstm_data(group, time_steps) for _, group in X_test_lstm_grouped]\n",
    "X_test_lstm_seq = [seq for seq in X_test_lstm_seq if len(seq) > 0]\n",
    "X_test_lstm = pd.concat(X_test_lstm_seq) if X_test_lstm_seq else pd.DataFrame()\n",
    "print(\"second is done\")\n",
    "\n",
    "X_train_lstm_seq = [create_lstm_data(group, time_steps) for _, group in X_train_lstm_grouped]     \n",
    "X_train_lstm_seq = [seq for seq in X_train_lstm_seq if len(seq) > 0]\n",
    "X_train_lstm = pd.concat(X_train_lstm_seq) if X_train_lstm_seq else pd.DataFrame()\n",
    "print(\"third is done\")\n",
    "\n",
    "X_val_lstm_seq = [create_lstm_data(group, time_steps) for _, group in X_val_lstm_grouped]\n",
    "X_val_lstm_seq = [seq for seq in X_val_lstm_seq if len(seq) > 0]\n",
    "X_val_lstm = pd.concat(X_val_lstm_seq) if X_val_lstm_seq else pd.DataFrame()\n",
    "\n",
    "y_in_sample_lstm_seq = [group[time_steps:] for _, group in y_in_sample_lstm_grouped]\n",
    "y_in_sample_lstm = pd.concat(y_in_sample_lstm_seq)\n",
    "y_test_lstm_seq = [group[time_steps:] for _, group in y_test_lstm_grouped]\n",
    "y_test_lstm = pd.concat(y_test_lstm_seq)\n",
    "y_train_lstm_seq = [group[time_steps:] for _, group in y_train_lstm_grouped]\n",
    "y_train_lstm = pd.concat(y_train_lstm_seq)\n",
    "y_val_lstm_seq = [group[time_steps:] for _, group in y_val_lstm_grouped]\n",
    "y_val_lstm = pd.concat(y_val_lstm_seq)\n",
    "\n",
    "def to_3d_numpy(df, time_steps, num_features):\n",
    "    # Convert dataframe to numpy array\n",
    "    data = df.values\n",
    "    # Get the number of stocks (samples)\n",
    "    num_samples = len(df) // time_steps\n",
    "    # Reshape the data to 3D for LSTM (num_samples, time_steps, num_features)\n",
    "    return data.reshape(num_samples, time_steps, num_features)\n",
    "\n",
    "# Convert and reshape\n",
    "if b_sentiment_score == True and n_past_returns == 1:\n",
    "    X_in_sample_lstm = to_3d_numpy(X_in_sample_lstm[[\"PreviousdayReturn\", \"SESI\"]], time_steps, 2)\n",
    "    X_test_lstm = to_3d_numpy(X_test_lstm[[\"PreviousdayReturn\", \"SESI\"]], time_steps, 2)\n",
    "    X_train_lstm = to_3d_numpy(X_train_lstm[[\"PreviousdayReturn\", \"SESI\"]], time_steps, 2)\n",
    "    X_val_lstm = to_3d_numpy(X_val_lstm[[\"PreviousdayReturn\", \"SESI\"]], time_steps, 2)\n",
    "elif b_sentiment_score == False and n_past_returns == 1:\n",
    "    X_in_sample_lstm = to_3d_numpy(X_in_sample_lstm[[\"PreviousdayReturn\"]], time_steps, 1)\n",
    "    X_test_lstm = to_3d_numpy(X_test_lstm[[\"PreviousdayReturn\"]], time_steps, 1)\n",
    "    X_train_lstm = to_3d_numpy(X_train_lstm[[\"PreviousdayReturn\"]], time_steps, 1)\n",
    "    X_val_lstm = to_3d_numpy(X_val_lstm[[\"PreviousdayReturn\"]], time_steps, 1)\n",
    "elif b_sentiment_score == True and n_past_returns == 3:\n",
    "    X_in_sample_lstm = to_3d_numpy(X_in_sample_lstm[[\"PreviousdayReturn\", \"PreviousdayReturn_2\", \"PreviousdayReturn_3\", \"SESI\"]], time_steps, 4)\n",
    "    X_test_lstm = to_3d_numpy(X_test_lstm[[\"PreviousdayReturn\", \"PreviousdayReturn_2\", \"PreviousdayReturn_3\", \"SESI\"]], time_steps, 4)\n",
    "    X_train_lstm = to_3d_numpy(X_train_lstm[[\"PreviousdayReturn\", \"PreviousdayReturn_2\", \"PreviousdayReturn_3\", \"SESI\"]], time_steps, 4)\n",
    "    X_val_lstm = to_3d_numpy(X_val_lstm[[\"PreviousdayReturn\", \"PreviousdayReturn_2\", \"PreviousdayReturn_3\", \"SESI\"]], time_steps, 4)\n",
    "elif b_sentiment_score == False and n_past_returns == 3:\n",
    "    X_in_sample_lstm = to_3d_numpy(X_in_sample_lstm[[\"PreviousdayReturn\", \"PreviousdayReturn_2\", \"PreviousdayReturn_3\"]], time_steps, 3)\n",
    "    X_test_lstm = to_3d_numpy(X_test_lstm[[\"PreviousdayReturn\", \"PreviousdayReturn_2\", \"PreviousdayReturn_3\"]], time_steps, 3)\n",
    "    X_train_lstm = to_3d_numpy(X_train_lstm[[\"PreviousdayReturn\", \"PreviousdayReturn_2\", \"PreviousdayReturn_3\"]], time_steps, 3)\n",
    "    X_val_lstm = to_3d_numpy(X_val_lstm[[\"PreviousdayReturn\", \"PreviousdayReturn_2\", \"PreviousdayReturn_3\"]], time_steps, 3)\n",
    "\n",
    "y_in_sample_lstm = y_in_sample_lstm[[\"Target\"]].values\n",
    "y_test_lstm = y_test_lstm[[\"Target\"]].values\n",
    "y_train_lstm = y_train_lstm[[\"Target\"]].values\n",
    "y_val_lstm = y_val_lstm[[\"Target\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 8min\n",
    "\n",
    "# Unclear if this is needed, maybe using 3 lags with time steps is also good, test this\n",
    "import LSTM\n",
    "importlib.reload(LSTM)\n",
    "\n",
    "# Creates splits and features and target dataframes\n",
    "X_in_sample, X_test, y_in_sample, y_test, X_train, X_val, y_train, y_val = Data_preparation.create_splits(df, percentage_split_number, model, b_sentiment_score)\n",
    "\n",
    "# Type should be multi_index_dataframe or 3D_array\n",
    "X_in_sample_lstm, X_test_lstm, y_in_sample_lstm, y_test_lstm, X_train_lstm, X_val_lstm, y_train_lstm, y_val_lstm = LSTM.data_preparation(X_in_sample, X_test, y_in_sample, y_test, X_train, X_val, y_train, y_val, time_steps, b_sentiment_score, n_past_returns)\n",
    "\n",
    "# Check if df_accuracy exists, if not create it\n",
    "try:\n",
    "    df_accuracy\n",
    "except NameError:\n",
    "    # Creates dataframe which will be filled with all Accuracy values\n",
    "    df_accuracy = pd.DataFrame({}, index=[\"All\", \"Top 10\", \"Top 20\", \"Top decile\", \"Top quintile\", \"Second quintile\", \"Third quintile\", \"Fourth quintile\", \"Bottom quintile\", \"Bottom decile\", \"Bottom 20\", \"Bottom 10\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Turn of printing for PC1??? It prints a lot of shit \n",
    "# 185min for PC1, with SESI\n",
    "\n",
    "# Takes #Epochs * #Tuning parameters * 1min to run  approximately\n",
    "\n",
    "import LSTM\n",
    "importlib.reload(LSTM)\n",
    "\n",
    "# Specify the grid \n",
    "# 18ms/step with dropout=0.1 and recurrent_dropout=0.1\n",
    "# 8ms/step with dropout=0.1 and recurrent_dropout=0.0\n",
    "grid ={\"dropout\": [0.1], # Fischer uses 0.1                                                                             # Not much difference   \n",
    "        \"recurrent_dropout\": [0.1],     #[0, 0.1], # Fischer uses 0.1 (after hyperparameter tuning)                                                                   # Adding recurrent_dropout makes the model 2.25 times slower   \n",
    "        \"learning_rate\": [0.001],          #[0.1, 0.01, 0.001], # 0.001 is default for RMSprop and adam (also seemed best)                                              # learning_rate smaller should take longer to run but I do not see any difference here for values  0.1, 0.01, 0.001, 0.0001\n",
    "        \"batch_size\": [32],     #[32, 64], # Default is 32                                                                             # Twice as fast with a twice as big batch size\n",
    "        \"optimizer\" : [\"RMSprop\"], # RMSprop Not default but used bij Fisher and good for RNN, default is adam             # adam seems eually fast as RMSprop                \n",
    "        \"sequence_length\": [time_steps] # 250 is roughly 1 year of data\n",
    "        }              \n",
    "\n",
    "# Make custom loss metric or evaluation metric for tuning\n",
    "\n",
    "\n",
    "# Trains model on validation set\n",
    "performances_LSTM, best_model_LSTM = LSTM.LSTM_tune(X_train_lstm, y_train_lstm, X_val_lstm, y_val_lstm, grid, b_sentiment_score, n_past_returns) \n",
    "\n",
    "# Plot the training and validation loss for the best model also return the best model\n",
    "LSTM.plot_train_val_loss(performances_LSTM)        #, tuning_metric = \"val_loss\") # tuning_metric = \"val_loss\" or \"val_accuracy\"\n",
    "\n",
    "\n",
    "# Hoe lang duurt 1 LSTM loop???????????????? Dit is met 1 lookback -> Aan de hand hiervan Tuning par bepalen   -> 14min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of rows in df_classification, df_predictions and df_classifications_lstm, df_predictions_lstm are not the same yet due to the time_steps\n",
    "\n",
    "# Creates dataframe which will be filled with all predictions\n",
    "df_classifications_lstm = df.loc[:, df.columns.isin([\"BarDate\", \"Ticker\", \"NextdayReturn\", \"Target\"])]\n",
    "df_classifications_lstm = df_classifications_lstm.tail(len(X_test_lstm))\n",
    "\n",
    "# Creates dataframe which will be filled with all classifications\n",
    "df_predictions_lstm = df.loc[:, df.columns.isin([\"BarDate\", \"Ticker\", \"NextdayReturn\", \"Target\"])]\n",
    "df_predictions_lstm = df_predictions_lstm.tail(len(X_test_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes #Epochs times 5 times 70sec to run\n",
    "\n",
    "import LSTM\n",
    "importlib.reload(LSTM)\n",
    "\n",
    "# Takes 160 minutes to run\n",
    "ar_predictions_lstm, ar_classifications_lstm, model_lstm = LSTM.LSTM_test(best_model_LSTM, window_size, y_in_sample_lstm, X_in_sample_lstm, y_test_lstm, X_test_lstm, b_sentiment_score, n_past_returns) \n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_predictions_lstm[\"predictions_lstm\"] = ar_predictions_lstm\n",
    "print(df_predictions_lstm[df_predictions_lstm[\"predictions_lstm\"] > 0.5].count())\n",
    "# Add predictions to df_predictions\n",
    "df_classifications_lstm[\"classifications_lstm\"] = ar_classifications_lstm\n",
    "print(df_classifications_lstm[df_classifications_lstm[\"classifications_lstm\"] == 1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "\n",
    "# Assuming `model_lstm` is your trained LSTM model\n",
    "# And `X_train` is your training data\n",
    "\n",
    "# We initialize the javascript for SHAP\n",
    "shap.initjs()\n",
    "\n",
    "# We create an explainer. DeepExplainer works well for deep learning models like LSTM.\n",
    "explainer = shap.DeepExplainer(model_lstm, X_train_lstm)\n",
    "\n",
    "# Generate SHAP values\n",
    "shap_values = explainer.shap_values(X_train_lstm)\n",
    "\n",
    "# Convert the input training set to a numpy array\n",
    "X_train_array = np.array(X_train_lstm)\n",
    "\n",
    "# Summarize the effects of all the features\n",
    "shap.summary_plot(shap_values, X_train_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `model_lstm` is your trained LSTM model\n",
    "# And `X_train` is your training data\n",
    "\n",
    "# We initialize the javascript for SHAP\n",
    "shap.initjs()\n",
    "\n",
    "# We create an explainer. DeepExplainer works well for deep learning models like LSTM.\n",
    "explainer = shap.DeepExplainer(model_lstm, X_train)\n",
    "\n",
    "# Generate SHAP values\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# Convert the input training set to a numpy array\n",
    "X_train_array = np.array(X_train)\n",
    "\n",
    "# Summarize the effects of all the features\n",
    "shap.summary_plot(shap_values, X_train_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.save('model_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "\n",
    "# loaded_model = load_model('model_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: you may need to restart the kernel to use updated packages.\n",
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model_lstm, to_file='model_lstm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique elements of SymbolExchangeCode for every BarDate\n",
    "test = X_test.groupby(\"BarDate\")[\"Ticker\"].nunique()\n",
    "\n",
    "# Plot the number of unique elements of SymbolExchangeCode for every BarDate\n",
    "X_test.groupby(\"BarDate\")[\"Ticker\"].nunique().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Evaluation_metrics\n",
    "importlib.reload(Evaluation_metrics)\n",
    "\n",
    "predictions_accuracy = Evaluation_metrics.prediction_metrics(df_classifications_lstm, str_model = \"lstm\", str_single_metric = \"Accuracy\")\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_accuracy[\"LSTM\"] = predictions_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio backtests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Portfolio part needs to be adjusted, seminar model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_only = True\n",
    "# short_only = False\n",
    "\n",
    "# # Number of long and short stocks\n",
    "# n_both = 20\n",
    "# n_long = n_both\n",
    "# n_short = n_both\n",
    "\n",
    "# Choose which models we want\n",
    "backtest_models = [\"lr\", \"rf\", \"gbc\", \"lstm\"]                               #[\"lr\", \"rf\", \"gbc\", \"lstm]                #\"NN1\", \"NN2\", \"NN3\", \"NN4\", \"NN5\"] \n",
    "\n",
    "# Choose weighting method (Equal weighted (Equal) or RankBased weighted (RankBased))\n",
    "# weighting_method = \"Equal\" \n",
    "\n",
    "# Initialize to be filled dataframe with portfolio returns\n",
    "df_returns_portfolio = pd.DataFrame({\"BarDate\": df_predictions[\"BarDate\"].unique()})\n",
    "# Initialize to be filled dataframe with portfolio returns\n",
    "df_returns_portfolio_lstm = pd.DataFrame({\"BarDate\": df_predictions_lstm[\"BarDate\"].unique()})\n",
    "\n",
    "dict_df_metrics = {}\t\n",
    "# # Initialize to be filled dataframe with portfolio metrics\n",
    "# df_metrics = pd.DataFrame({}, index=['total_return', \"mean_return\", 'std_dev', 'information_ratio', \"appraisal_ratio\",\n",
    "#                                       'max_drawdown', 'max_1_month_loss', 'max_1_year_loss', \"turnover\"])\n",
    "\n",
    "# # Cumulative return of stock index, used only in plot\n",
    "# stockindex_returns_cum = (1 + df_stockindex_returns[\"Return\"]).cumprod() - 1\n",
    "df_stockindex_returns = None\n",
    "\n",
    "# Define the colors for the lines\n",
    "colors = ['red', 'blue', 'green', 'orange', 'red', 'blue', 'green', 'orange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 5:30 to run\n",
    "\n",
    "# Creates a matrix with all actual returns only \n",
    "# The last columns are not the last dates. These all seem NaN values.............Something must change...........................\n",
    "stocks = df['Ticker'].unique()\n",
    "returnsMatrix = pd.DataFrame(index=df_predictions['BarDate'].unique(), columns=stocks)\n",
    "for stock in stocks:\n",
    "    stock_df = df[df['Ticker'] == stock]\n",
    "    for index, row in stock_df.iterrows():\n",
    "        returnsMatrix.at[row['BarDate'], stock] = row['NextdayReturn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra metrics take long to run\n",
    "# Takes +-12min to run\n",
    "\n",
    "import Portfolio_backtester\n",
    "importlib.reload(Portfolio_backtester)\n",
    "\n",
    "# Number of long and short stocks\n",
    "# for n_both in tqdm([10, 20, 50]):\n",
    "\n",
    "n_both = 20\n",
    "n_long = n_both\n",
    "n_short = n_both\n",
    "n_constituents = n_both * 2\n",
    "# Create a loop which first sets long_only = True and then short_only = True and then both = False\n",
    "for long_only in tqdm([True, False]):\n",
    "    for short_only in [True, False]:\n",
    "        if (long_only == True) & (short_only == True):\n",
    "            continue\n",
    "\n",
    "        # Plot the cumulative portfolio return\n",
    "        plt.subplots(figsize=(20,10))\n",
    "        # Dictionary to be filled with weight matrices\n",
    "        weight_matrix_dict = {}\n",
    "        # Use this for color of the lines\n",
    "        i = 0\n",
    "        for weighting_method in tqdm([\"Equal\", \"RankBased\"]):\n",
    "            dict_df_metrics[f\"{weighting_method}_{n_both}\"] = pd.DataFrame({}, index=['total_return', \"mean_return\", 'std_dev', 'information_ratio', \"appraisal_ratio\", 'max_drawdown', 'max_1_month_loss', 'max_1_year_loss', \"turnover\"])\n",
    "            for backtest_model in tqdm(backtest_models):\n",
    "                # Predictions are now stored in two different objects\n",
    "                if backtest_model == \"lstm\":\n",
    "                    df_predictions_backtest = df_predictions_lstm\n",
    "                    df_returns_portfolio_backtest = df_returns_portfolio_lstm\n",
    "                else:\n",
    "                    df_predictions_backtest = df_predictions\n",
    "                    df_returns_portfolio_backtest = df_returns_portfolio\n",
    "\n",
    "                predictions_test_set, df_returns_portfolio_backtest, str_returns_portfolio_cum, str_logreturns_portfolio_cum, weightMatrix = Portfolio_backtester.backtest(df_predictions_backtest,n_long,n_short,backtest_model,\n",
    "                                                                                weighting_method, df_returns_portfolio_backtest, returnsMatrix, long_only, short_only)\n",
    "                weight_matrix_dict.update({backtest_model: weightMatrix})\n",
    "          \n",
    "                if weighting_method == \"Equal\":\n",
    "                    plt.plot(pd.to_datetime(df_returns_portfolio_backtest[\"BarDate\"]), df_returns_portfolio_backtest[str_logreturns_portfolio_cum], color = colors[i], label=str(f\"{backtest_model} {weighting_method}\"))\n",
    "                elif(weighting_method == \"RankBased\"):\n",
    "                    plt.plot(pd.to_datetime(df_returns_portfolio_backtest[\"BarDate\"]), df_returns_portfolio_backtest[str_logreturns_portfolio_cum], color = colors[i], linestyle = '--', label=str(f\"{backtest_model} {weighting_method}\"))\n",
    "                # Gives metrics\n",
    "                dict_df_metrics[f\"{weighting_method}_{n_both}\"] = Portfolio_backtester.metrics(df_returns_portfolio_backtest, backtest_model, dict_df_metrics[f\"{weighting_method}_{n_both}\"], df_stockindex_returns, returnsMatrix, weightMatrix)\n",
    "                i = i + 1\n",
    "\n",
    "        #plt.plot(pd.to_datetime(df_stockindex_returns[\"Date\"]), stockindex_returns_cum) # Plots long only index\n",
    "        plt.axvspan(datetime.datetime.strptime(\"29/02/2020\", '%d/%m/%Y').date(), datetime.datetime.strptime(\"31/03/2020\", '%d/%m/%Y').date(), color=\"grey\", alpha=0.5)\n",
    "        # plt.axvspan(datetime.datetime.strptime(\"31/12/2007\", '%d/%m/%Y').date(), datetime.datetime.strptime(\"31/05/2009\", '%d/%m/%Y').date(), color=\"grey\", alpha=0.5)\n",
    "\n",
    "        plt.xlabel(xlabel=\"Date\", fontsize = \"x-large\")\n",
    "        if long_only == True:\n",
    "            plt.ylabel(ylabel=\"Cumulative portfolio (long) logreturns\", fontsize = \"x-large\")\n",
    "        if short_only == True:\n",
    "            plt.ylabel(ylabel=\"Cumulative portfolio (short) logreturns\", fontsize = \"x-large\")\n",
    "        if (long_only == False) & (short_only == False):\n",
    "            plt.ylabel(ylabel=\"Cumulative portfolio logreturns\", fontsize = \"x-large\")\n",
    "        plt.title(label=f\"Cumulative {n_constituents}-stock portfolio logreturns\", fontsize = \"xx-large\")  \n",
    "        plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "        plt.legend(['lr', 'rf', 'gb', 'lstm', 'lr_rank', 'rf_rank', 'gb_rank', 'lstm_rank'] , loc = 'upper center', ncols = 4, fontsize = 'large')    # Without specifying loc it chooses the best location\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "print(dict_df_metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change for new file!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "b_sentiment_score \n",
    "stockindex = \"SPH\"\n",
    "LSTM_model_type = \"LSTM_model_1\"\n",
    "time_steps\n",
    "# n_both\n",
    "# weighting_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Creating the directory name\n",
    "directory = r\"C:\\Users\\BasPeeters\\OneDrive - FactorOrange.capital\\Master Thesis\\Dataframes and output\"\n",
    "folder_name = f\"{stockindex}_SESI={b_sentiment_score}_{LSTM_model_type}_time_steps = {time_steps}\"\n",
    "full_directory = os.path.join(directory, folder_name)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(full_directory):\n",
    "    os.makedirs(full_directory)\n",
    "\n",
    "# Save your dataframes using pickle\n",
    "df_classifications_lstm.to_pickle(os.path.join(full_directory, \"df_classifications_lstm.pkl\"))\n",
    "df_predictions_lstm.to_pickle(os.path.join(full_directory, \"df_predictions_lstm.pkl\"))\n",
    "df_predictions.to_pickle(os.path.join(full_directory, \"df_predictions.pkl\"))\n",
    "df_classifications.to_pickle(os.path.join(full_directory, \"df_classifications.pkl\"))\n",
    "\n",
    "# Save metrics and to csv\n",
    "df_accuracy.to_csv(os.path.join(full_directory, \"df_accuracy.csv\"))\n",
    "for key in dict_df_metrics:\n",
    "    dict_df_metrics[key].to_csv(os.path.join(full_directory, f'df_metrics_{key}.csv'))\n",
    "\n",
    "# try:\n",
    "#     df_returns_portfolio_lstm.to_pickle(os.path.join(full_directory, f\"df_returns_portfolio_lstm_n_stock={n_both}_weighting_method={weighting_method}.pkl\"))\n",
    "# except:\n",
    "#     print(\"df_returns_portfolio_lstm does not exist\")\n",
    "# try:\n",
    "#     df_returns_portfolio.to_pickle(os.path.join(full_directory, f\"df_returns_portfolio_n_stock={n_both}_weighting_method={weighting_method}.pkl\"))\n",
    "# except:\n",
    "#     print(\"df_returns_portfolio_ does not exist\")\n",
    "\n",
    "try: \n",
    "    # Save your models\n",
    "    dump(best_model_LSTM, os.path.join(full_directory, 'best_model_LSTM.joblib'))\n",
    "except:\n",
    "    print(\"best_model_LSTM does not exist\")\t\n",
    "try:\n",
    "    dump(best_model_GBC, os.path.join(full_directory, 'best_model_GBC.joblib'))\n",
    "except:\n",
    "    print(\"best_model_GBC does not exist\")\n",
    "try:\n",
    "    dump(best_model_RF, os.path.join(full_directory, 'best_model_RF.joblib'))\n",
    "except:\n",
    "    print(\"best_model_RF does not exist\")\n",
    "\n",
    "    \n",
    "print(f\"All data is saved to {full_directory}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "# Creating the directory name\n",
    "directory = r\"C:\\Users\\BasPeeters\\OneDrive - FactorOrange.capital\\Master Thesis\\Dataframes and output\"\n",
    "folder_name = f\"{stockindex}_SESI={b_sentiment_score}_{LSTM_model_type}_time_steps = {time_steps}\"\n",
    "full_directory = os.path.join(directory, folder_name)\n",
    "\n",
    "# dict_df_metrics = pd.read_pickle(os.path.join(full_directory, \"dict_df_metrics.pkl\"))\n",
    "df_classifications_lstm = pd.read_pickle(os.path.join(full_directory, \"df_classifications_lstm.pkl\"))\n",
    "df_predictions_lstm = pd.read_pickle(os.path.join(full_directory, \"df_predictions_lstm.pkl\"))\n",
    "df_predictions = pd.read_pickle(os.path.join(full_directory, \"df_predictions.pkl\"))\n",
    "df_classifications = pd.read_pickle(os.path.join(full_directory, \"df_classifications.pkl\"))\n",
    "# df_accuracy = pd.read_pickle(os.path.join(full_directory, \"df_accuracy.pkl\"))\n",
    "# df_returns_portfolio_lstm = pd.read_pickle(os.path.join(full_directory, f\"df_returns_portfolio_lstm_n_stock={n_both}_weighting_method={weighting_method}.pkl\"))\n",
    "# df_returns_portfolio = pd.read_pickle(os.path.join(full_directory, f\"df_returns_portfolio_n_stock={n_both}_weighting_method={weighting_method}.pkl\"))\n",
    "\n",
    "df_accuracy = pd.read_csv(os.path.join(full_directory, \"df_accuracy.csv\"))\n",
    "dict_df_metrics = {}\n",
    "for key in [\"Equal_20\", \"RankBased_20\"]:\n",
    "    dict_df_metrics[key] = pd.read_csv(os.path.join(full_directory, f'df_metrics_{key}.csv'))\n",
    "\n",
    "# best_model_LSTM = load(os.path.join(full_directory, \"best_model_LSTM.joblib\"))\n",
    "# best_model_GBC = load(os.path.join(full_directory, \"best_model_GBC.joblib\"))\n",
    "# best_model_RF = load(os.path.join(full_directory, \"best_model_RF.joblib\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have your data in the following format:\n",
    "# data = {\n",
    "#     'Portfolio1': {'10': {'Accuracy': 0.8, 'Sharpe Ratio': 1.2, 'Std Dev': 0.1, 'Return': 0.05},\n",
    "#                    '20': {'Accuracy': 0.75, 'Sharpe Ratio': 1.1, 'Std Dev': 0.15, 'Return': 0.06},\n",
    "#                    '30': {'Accuracy': 0.85, 'Sharpe Ratio': 1.3, 'Std Dev': 0.12, 'Return': 0.07}},\n",
    "#     'Portfolio2': {'10': {'Accuracy': 0.9, 'Sharpe Ratio': 1.4, 'Std Dev': 0.11, 'Return': 0.08},\n",
    "#                    '20': {'Accuracy': 0.88, 'Sharpe Ratio': 1.5, 'Std Dev': 0.13, 'Return': 0.09},\n",
    "#                    '30': {'Accuracy': 0.92, 'Sharpe Ratio': 1.6, 'Std Dev': 0.14, 'Return': 0.1}},\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "data = {\n",
    "    'Portfolio1': {'10': {'Accuracy': 0.8, 'Sharpe Ratio': 1.2, 'Std Dev': 0.1, 'Return': 0.05},\n",
    "                   '20': {'Accuracy': 0.75, 'Sharpe Ratio': 1.1, 'Std Dev': 0.15, 'Return': 0.06},\n",
    "                   '30': {'Accuracy': 0.85, 'Sharpe Ratio': 1.3, 'Std Dev': 0.12, 'Return': 0.07}},\n",
    "    'Portfolio2': {'10': {'Accuracy': 0.9, 'Sharpe Ratio': 1.4, 'Std Dev': 0.11, 'Return': 0.08},\n",
    "                   '20': {'Accuracy': 0.88, 'Sharpe Ratio': 1.5, 'Std Dev': 0.13, 'Return': 0.09},\n",
    "                   '30': {'Accuracy': 0.92, 'Sharpe Ratio': 1.6, 'Std Dev': 0.14, 'Return': 0.1}},\n",
    "    # Add more portfolios as needed\n",
    "}\n",
    "\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']  # Add more colors if you have more than 7 portfolios\n",
    "\n",
    "fig, ax = plt.subplots(4, 3, figsize=(10, 10))\n",
    "\n",
    "# Plotting\n",
    "for i, metric in enumerate(['Accuracy', 'Sharpe Ratio', 'Std Dev', 'Return']):\n",
    "    for j, num_stocks in enumerate(['10', '20', '30']):\n",
    "        ax[i, j].bar(data.keys(), [v[num_stocks][metric] for v in data.values()], color=colors)\n",
    "        if i == 0:\n",
    "            ax[i, j].set_title(f'{num_stocks} stocks')\n",
    "        if j == 0:\n",
    "            ax[i, j].set_ylabel(metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
