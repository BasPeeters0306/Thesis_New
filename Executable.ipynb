{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import importlib\n",
    "from warnings import simplefilter\n",
    "from six import StringIO\n",
    "from IPython.display import Image\n",
    "\n",
    "# Reload external files\n",
    "import importlib\n",
    "\n",
    "# Mute warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change function of EOD because we use Compustat....\n",
    "# In the end, add all used Excel files to this repo, we have: stock_universe_new.joblib, SP500H_EOD_raw, SP500H_RP_SESI (if we only use daily SESI score and no other sentiment feature types)\n",
    "# We use EOD for the list of historical components\n",
    "# At the end, just do all data steps one more time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose data location, EOD, SQL or CSV, Compustat\n",
    "str_load_data = \"CSV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This entire notebook should go in a separate ipynb file later\n",
    "\n",
    "# Loads raw data, data cleaning and transformation is needed after this\n",
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "\n",
    "if str_load_data == \"EOD_old_method\":\n",
    "    \n",
    "    # Index name \n",
    "    str_index_name = \"S&P500_historical\"\n",
    "\n",
    "    # Loads market data from EOD\n",
    "    ar_index, df_index = Load_data.create_stock_universe_old(str_index_name)\n",
    "    df = Load_data.load_EOD(ar_index)\n",
    "\n",
    "    df = Load_data.create_historical_SP_Index_old_method(df_index, df)            \n",
    "\n",
    "    # Adds lags and returns\n",
    "    df = Load_data.add_lags(df)\n",
    "\n",
    "if str_load_data == \"EOD_new_method\":\n",
    "\n",
    "    df = Load_data.create_historical_SP_Index()\n",
    "\n",
    "    # Adds lags and returns\n",
    "    df = Load_data.add_lags(df)\n",
    "\n",
    "elif str_load_data == \"SQL\":\n",
    "    \n",
    "    # Name of the dataframe\n",
    "    str_table_name = \"SP500H_RP_raw\"\n",
    "\n",
    "    # Specify column subset, None if all columns should be loaded\n",
    "    str_column_subset = \"[TIMESTAMP_TZ], [RP_ENTITY_ID], [ENTITY_NAME], [EVENT_SENTIMENT], [EVENT_RELEVANCE], [EVENT_SIMILARITY_DAYS]\"\n",
    "\n",
    "    # Loads the dataframe from SQL\n",
    "    df = Load_data.load_SQL(str_table_name, str_column_subset)\n",
    "\n",
    "elif str_load_data == \"CSV\":\n",
    "\n",
    "    df_full = Load_data.load_csv(\"df_Compustat_SPH\")\n",
    "\n",
    "# Check the number of unique elements of SymbolExchangeCode for every BarDate\n",
    "# df_test = df.groupby(\"BarDate\")[\"Ticker\"].nunique()\n",
    "\n",
    "# Plot the number of unique elements of SymbolExchangeCode for every BarDate\n",
    "# df.groupby(\"BarDate\")[\"Ticker\"].nunique().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all possible places to df when ready....... (to save space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "# Loads raw Compustat data from CSV\n",
    "df_Compustat_raw = Load_data.load_csv(\"df_Compustat_raw\")\n",
    "df_Compustat_raw.groupby(\"BarDate\")[\"Ticker\"].nunique().plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Historical SP500 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "df_Compustat_raw_SPH = Load_data.create_historical_SP_Index_deliver(df_Compustat_raw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add lags, returns and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "df_Compustat_raw_SPH_variables = Load_data.add_variables(df_Compustat_raw_SPH)\n",
    "df_Compustat_raw_SPH_variables.groupby(\"BarDate\")[\"Ticker\"].nunique().plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "# Loads raw RavenPack data from CSV\n",
    "df_RP_raw = Load_data.load_csv(\"df_RP_raw\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SESI score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SESI\n",
    "importlib.reload(SESI)\n",
    "# Creates daily SESI score from raw RavenPack data\n",
    "df_SESI = SESI.SESI(df_RP_raw)\n",
    "# Adds Ticker column to df_SESI\n",
    "df_SESI = SESI.add_ticker(df_SESI)\n",
    "df_SESI[\"TIMESTAMP_TZ\"] = pd.to_datetime(df_SESI[\"TIMESTAMP_TZ\"])\n",
    "df_SESI.groupby(\"TIMESTAMP_TZ\")[\"Ticker\"].nunique().plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge df_SESI and df_market to df and add lagged SESI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes long to run, +-1100min\n",
    "# Was very quick with new function (only 3sec)\n",
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "df = Load_data.add_SESI(df_Compustat_raw_SPH_variables, df_SESI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load or Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "file_name = \"df\"\n",
    "df = Load_data.load_csv(file_name)\n",
    "\n",
    "# import Save_data\n",
    "# importlib.reload(Save_data)\n",
    "# file_name = \"df_Compustat_raw_SPH\"\n",
    "# Save_data.save_to_csv(df_Compustat_raw_SPH, file_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Analyze_data\n",
    "importlib.reload(Analyze_data)\n",
    "\n",
    "# Check the number of unique elements of SymbolExchangeCode for every BarDate\n",
    "Analyze_data.unique_stocks_by_date(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Analyze_data\n",
    "importlib.reload(Analyze_data)\n",
    "\n",
    "# Plot all stocks\n",
    "Analyze_data.plot_all_stocks(df, max_AdjustedClose = 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show summary statistics for CS_MedianNextDayReturn\n",
    "print(df['CS_MedianNextdayReturn'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which metrics you want to check\n",
    "\n",
    "bool_dict = {\n",
    "    'descriptive_stats': True,\n",
    "    'correlation_matrix': False,\n",
    "    \"histogram\": False,\n",
    "    \"boxplot\": False,\n",
    "    \"scatterplot\": False,\n",
    "    \"lineplot\": False,\n",
    "    \"heatmap\": False,\n",
    "    \"barplot\": False,\n",
    "    \"piechart\": False,\n",
    "    \"violinplot\": False,\n",
    "    \"kdeplot\": False,\n",
    "    \"hexbinplot\": False,\n",
    "    \"scatter_matrix\": False,\n",
    "    \"parallel_coordinates\": False,\n",
    "    \"andrews_curves\": False,\n",
    "    \"radviz\": False,\n",
    "    \"lag_plot\": False,\n",
    "    \"autocorrelation_plot\": False,\n",
    "    \"bootstrap_plot\": False\n",
    "}\n",
    "\n",
    "import Analyze_data\n",
    "importlib.reload(Analyze_data)\n",
    "\n",
    "Analyze_data.Data_analysis(df, bool_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import importlib\n",
    "from warnings import simplefilter\n",
    "from six import StringIO\n",
    "from IPython.display import Image\n",
    "\n",
    "# Reload external files\n",
    "import importlib\n",
    "\n",
    "# Mute warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "file_name = \"df\"\n",
    "df = Load_data.load_csv(file_name)\n",
    "df[\"BarDate\"] = pd.to_datetime(df[\"BarDate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Order df by BarDate and SymbolExchangeCode\n",
    "# df = df.sort_values(by = [\"BarDate\", \"Ticker\"])\n",
    "# # reset index\n",
    "# df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 'Hyperparameter' is the sequence_length/time_steps\n",
    "time_steps = 3\n",
    "b_sentiment_score = True\n",
    "n_past_returns = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_split_number = 0.50 # With the larger SP500H index we might be able to get back to 2008 with a reasonable percentage (0.40 min)\n",
    "b_MinMaxScaler = True \n",
    "b_standardizer = False \n",
    "\n",
    "import Data_preparation\n",
    "importlib.reload(Data_preparation)\n",
    "\n",
    "# Scale features with MinMaxScaler or standardize features with StandardScaler\n",
    "df = Data_preparation.feature_scaling(df, b_MinMaxScaler, b_standardizer, percentage_split_number)\n",
    "\n",
    "# Creates splits and features and target dataframes\n",
    "X_in_sample, X_test, y_in_sample, y_test, X_train, X_val, y_train, y_val = Data_preparation.create_splits(df, percentage_split_number, model = \"not_lstm\", b_sentiment_score = b_sentiment_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dataframe which will be filled with all predictions\n",
    "df_classifications = df.loc[:, df.columns.isin([\"BarDate\", \"Ticker\", \"NextdayReturn\", 'Target'])]\n",
    "df_classifications = df_classifications.tail(len(X_test))\n",
    "\n",
    "# Creates dataframe which will be filled with all classifications\n",
    "df_predictions = df.loc[:, df.columns.isin([\"BarDate\", \"Ticker\", \"NextdayReturn\", 'Target'])]\n",
    "df_predictions = df_predictions.tail(len(X_test))\n",
    "\n",
    "# Creates dataframe which will be filled with all Accuracy values\n",
    "df_accuracy = pd.DataFrame({}, index=[\"All\", \"Top 10\", \"Top 20\", \"Top decile\", \"Top quintile\", \"Second quintile\", \"Third quintile\", \"Fourth quintile\", \"Bottom quintile\", \"Bottom decile\", \"Bottom 20\", \"Bottom 10\"])\n",
    "# Expanding window size for predictions in the test set +- 1 year\n",
    "window_size = 500*500\n",
    "\n",
    "# Change dtypes to 32 for memory efficiency\n",
    "df['PreviousdayReturn'] = df['PreviousdayReturn'].astype('float32')\n",
    "df['PreviousdayReturn_2'] = df['PreviousdayReturn_2'].astype('float32')\n",
    "df['PreviousdayReturn_3'] = df['PreviousdayReturn_3'].astype('float32')\n",
    "df[\"Target\"] = df[\"Target\"].astype('float32')\n",
    "df[\"SESI\"] = df[\"SESI\"].astype('float32')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Logistic_regression\n",
    "importlib.reload(Logistic_regression)\n",
    "\n",
    "ar_classifications_lr, ar_predictions_lr, model_lr = Logistic_regression.logistic_regression(window_size, y_in_sample, X_in_sample, y_test, X_test)\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_predictions[\"predictions_lr\"] = ar_predictions_lr\n",
    "# Add classifications to df_classifications\n",
    "df_classifications[\"classifications_lr\"] = ar_classifications_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"coefficients\", model_lr.coef_)\n",
    "print(\"intercept\", model_lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Evaluation_metrics\n",
    "importlib.reload(Evaluation_metrics)\n",
    "\n",
    "predictions_accuracy = Evaluation_metrics.prediction_metrics(df_classifications, str_model = \"lr\", str_single_metric = \"Accuracy\")\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_accuracy[\"Logistic Regresion\"] = predictions_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with ENet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ENet\n",
    "# importlib.reload(ENet)\n",
    "\n",
    "# # Specify the grid\n",
    "# grid = {\"l1_ratio\": [0.3, 0.5, 0.7], \"C\": np.logspace(start=-40, stop=-10, num=10, base=10)}\n",
    "# # Predictions \n",
    "# performances_ENet, best_model_ENet = ENet.ENet_tune(X_train, y_train, X_val, y_val, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # At the moment this returns 50% proba always, not sure if ENEt a good model for our features\n",
    "# # Or something is wrong with the code\n",
    "\n",
    "# import ENet\n",
    "# importlib.reload(ENet)\n",
    "\n",
    "# ar_classifications_enet, ar_predictions_enet, model_enet = ENet.ENet_test(best_model_ENet, window_size, y_in_sample, X_in_sample, y_test, X_test)\n",
    "\n",
    "# # Add predictions to df_predictions\n",
    "# df_predictions[\"predictions_enet\"] = ar_predictions_enet\n",
    "# # Add classifications to df_classifications\n",
    "# df_classifications[\"classifications_enet\"] = ar_classifications_enet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Evaluation_metrics\n",
    "# importlib.reload(Evaluation_metrics)\n",
    "\n",
    "# predictions_accuracy = Evaluation_metrics.prediction_metrics(df_classifications, str_model = \"enet\", str_single_metric = \"Accuracy\")\n",
    "\n",
    "# # Add predictions to df_predictions\n",
    "# df_accuracy[\"Elastic Net\"] = predictions_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "\n",
    "# Should be 1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 234min for PC1, (with tuning 4*4 combinations), with SESI\n",
    "# 120min for PC3, Without SESI\n",
    "# 160min for PC2, Without SESI\n",
    "\n",
    "import RF\n",
    "importlib.reload(RF)\n",
    "\n",
    "# Specify the grid \n",
    "# n_jobs = -1 makes use of all cores\n",
    "# When setting max_depth > 2 and more then 100000 rows, the kernel crashes\n",
    "# Try a different RF package?\n",
    "grid = {\"n_estimators\": [500, 700, 900],   \n",
    "        \"max_depth\": [2, 4, 6, 8]}\n",
    "                                                  #\"max_features\":    [3, 5, 10]} # Not needed because we only have 4 features\n",
    "# Predictions \n",
    "performances_RF, best_model_RF =  RF.RF_tune(X_train, y_train, X_val, y_val, grid)   #RF.RF_tune(X_train_1000, y_train_1000, X_val_1000, y_val_1000, grid)                       #RF.RF_tune(X_train, y_train, X_val, y_val, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 213min for PC1, with SESI\n",
    "# 120min for PC3, Without SESI\n",
    "# 220min for PC2, Without SESI\n",
    "\n",
    "import RF\n",
    "importlib.reload(RF)\n",
    "\n",
    "ar_classifications_rf, ar_predictions_rf, model_rf = RF.RF_test(best_model_RF, window_size, y_in_sample, X_in_sample, y_test, X_test)\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_predictions[\"predictions_rf\"] = ar_predictions_rf\n",
    "# Add classifications to df_classifications\n",
    "df_classifications[\"classifications_rf\"] = ar_classifications_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Evaluation_metrics\n",
    "importlib.reload(Evaluation_metrics)\n",
    "\n",
    "predictions_accuracy = Evaluation_metrics.prediction_metrics(df_classifications, str_model = \"rf\", str_single_metric = \"Accuracy\")\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_accuracy[\"Random Forest\"] = predictions_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 210min for PC1, with SESI\n",
    "# 160min for PC3, without SESI\n",
    "# 180min for PC2, Without SESI\n",
    "\n",
    "import GBC\n",
    "importlib.reload(GBC)\n",
    "\n",
    "# Specify the grid \n",
    "grid = {\"n_estimators\": [100, 200, 500],\n",
    "        \"learning_rate\":[0.01, 0.1, 0.2],\n",
    "        \"max_depth\": [1, 2, 3]}\n",
    "# Predictions \n",
    "performances_GBC, best_model_GBC = GBC.GBC_tune(X_train, y_train, X_val, y_val, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... min for PC1, with SESI\n",
    "# 35 min for PC3, Without SESI\n",
    "# 40min for PC2, Without SESI\n",
    "\n",
    "import GBC\n",
    "importlib.reload(GBC)\n",
    "\n",
    "ar_classifications_gbc, ar_predictions_gbc, model_gbc = GBC.GBC_test(best_model_GBC, window_size, y_in_sample, X_in_sample, y_test, X_test)\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_predictions[\"predictions_gbc\"] = ar_predictions_gbc\n",
    "# Add classifications to df_classifications\n",
    "df_classifications[\"classifications_gbc\"] = ar_classifications_gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Evaluation_metrics\n",
    "importlib.reload(Evaluation_metrics)\n",
    "\n",
    "predictions_accuracy = Evaluation_metrics.prediction_metrics(df_classifications, str_model = \"gbc\", str_single_metric = \"Accuracy\")\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_accuracy[\"Gradient Boosting Classifier\"] = predictions_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (and SimpleRNN) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load LSTM data from pickles\n",
    "directory = r\"C:\\Users\\BasPeeters\\OneDrive - FactorOrange.capital\\Master Thesis\\Dataframes and output\"\n",
    "folder_name = f\"lstm_sequences\\SESI={b_sentiment_score}_time_steps={time_steps}\"\n",
    "full_directory = os.path.join(directory, folder_name)\n",
    "\n",
    "names = ['X_in_sample_lstm', 'X_test_lstm', 'y_in_sample_lstm', 'y_test_lstm', 'X_train_lstm', 'X_val_lstm', 'y_train_lstm', 'y_val_lstm', \"df_y_test_lstm\"]\n",
    "variables = []\n",
    "for name in names:\n",
    "    with open(os.path.join(full_directory, f\"{name}.pkl\"), 'rb') as f:\n",
    "        var = pickle.load(f)\n",
    "        variables.append(var)\n",
    "\n",
    "X_in_sample_lstm, X_test_lstm, y_in_sample_lstm, y_test_lstm, X_train_lstm, X_val_lstm, y_train_lstm, y_val_lstm, df_y_test_lstm = variables\n",
    "del variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Takes 10min\n",
    "\n",
    "# # Unclear if this is needed, maybe using 3 lags with time steps is also good, test this\n",
    "# import LSTM\n",
    "# importlib.reload(LSTM)\n",
    "\n",
    "# # Creates splits and features and target dataframes\n",
    "# model = \"lstm\"\n",
    "# X_in_sample, X_test, y_in_sample, y_test, X_train, X_val, y_train, y_val = Data_preparation.create_splits(df, percentage_split_number, model, b_sentiment_score)\n",
    "\n",
    "# # Type should be multi_index_dataframe or 3D_array\n",
    "# X_in_sample_lstm, X_test_lstm, y_in_sample_lstm, y_test_lstm, X_train_lstm, X_val_lstm, y_train_lstm, y_val_lstm, df_y_test_lstm = LSTM.data_preparation(X_in_sample, X_test, y_in_sample, y_test, X_train, X_val, y_train, y_val, time_steps, b_sentiment_score, n_past_returns)\n",
    "\n",
    "# # Check if df_accuracy exists, if not create it\n",
    "# try:\n",
    "#     df_accuracy\n",
    "# except NameError:\n",
    "#     # Creates dataframe which will be filled with all Accuracy values\n",
    "#     df_accuracy = pd.DataFrame({}, index=[\"All\", \"Top 10\", \"Top 20\", \"Top decile\", \"Top quintile\", \"Second quintile\", \"Third quintile\", \"Fourth quintile\", \"Bottom quintile\", \"Bottom decile\", \"Bottom 20\", \"Bottom 10\"])\n",
    "\n",
    "\n",
    "# the X and y sets must be of equal length\n",
    "print(len(X_val_lstm))\n",
    "print(len(X_train_lstm))\n",
    "print(len(X_test_lstm))\n",
    "print(len(X_in_sample_lstm))\n",
    "\n",
    "print(len(y_val_lstm))\n",
    "print(len(y_train_lstm))\n",
    "print(len(y_test_lstm))\n",
    "print(len(y_in_sample_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The custom loss is not yet entirely perfect, now it considers batches of 500 stocks and calculates deciles from these 500, but not necessarily on 1 time period\n",
    "loss_function = 'binary_crossentropy'  # or \"custom_loss\"\n",
    "# Type of RNN: \"SimpleRNN\", \"LSTM\" or \"GRU\"\n",
    "str_nn_type = \"SimpleRNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn of printing for PC1??? It prints a lot of shit \n",
    "# 185min for PC1, with SESI\n",
    "\n",
    "# Takes #Epochs * #Tuning parameters * 1min to run  approximately\n",
    "\n",
    "import LSTM\n",
    "importlib.reload(LSTM)\n",
    "\n",
    "# Specify the grid \n",
    "# 18ms/step with dropout=0.1 and recurrent_dropout=0.1\n",
    "# 8ms/step with dropout=0.1 and recurrent_dropout=0.0\n",
    "grid ={ \"n_nodes\": [25],\n",
    "        \"dropout\": [0.1], # Fischer uses 0.1                                                                             # Not much difference   \n",
    "        \"recurrent_dropout\": [0],     #[0, 0.1], # Fischer uses 0.1 (after hyperparameter tuning)                                                                   # Adding recurrent_dropout makes the model 2.25 times slower   \n",
    "        \"learning_rate\": [0.001],          #[0.1, 0.01, 0.001], # 0.001 is default for RMSprop and adam (also seemed best)                                              # learning_rate smaller should take longer to run but I do not see any difference here for values  0.1, 0.01, 0.001, 0.0001\n",
    "        \"batch_size\": [64],     #[32, 64], # Default is 32                                                                              # Twice as fast with a twice as big batch size\n",
    "        \"optimizer\" : [\"RMSprop\"], # RMSprop Not default but used bij Fisher and good for RNN, default is adam             # adam seems eually fast as RMSprop                \n",
    "        \"sequence_length\": [time_steps] # 250 is roughly 1 year of data\n",
    "        } \n",
    "\n",
    "# Because the loss function needs this\n",
    "if loss_function == \"custom_loss\":\n",
    "    grid[\"batch_size\"] = [500]\n",
    "\n",
    "if str_nn_type == \"LSTM\":\n",
    "    # Trains model on validation set\n",
    "    performances_LSTM, best_model_LSTM = LSTM.LSTM_tune(X_train_lstm, y_train_lstm, X_val_lstm, y_val_lstm, grid, b_sentiment_score, n_past_returns, loss_function, str_nn_type) \n",
    "    # Plot the training and validation loss for the best model also return the best model\n",
    "    LSTM.plot_train_val_loss(performances_LSTM)    \n",
    "if str_nn_type == \"SimpleRNN\":\n",
    "    # Trains model on validation set\n",
    "    performances_RNN, best_model_RNN = LSTM.LSTM_tune(X_train_lstm, y_train_lstm, X_val_lstm, y_val_lstm, grid, b_sentiment_score, n_past_returns, loss_function, str_nn_type) \n",
    "    # Plot the training and validation loss for the best model also return the best model\n",
    "    LSTM.plot_train_val_loss(performances_RNN)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if str_nn_type == \"LSTM\":\n",
    "    # Creates dataframe which will be filled with all predictions\n",
    "    df_classifications_lstm = df_y_test_lstm.copy()\n",
    "    # Join the column NextdayReturn from df to df_classifications_lstm by BarDate and Ticker\n",
    "    df_classifications_lstm = df_classifications_lstm.join(df.loc[:, df.columns.isin([\"BarDate\", \"Ticker\", \"NextdayReturn\"])].set_index([\"BarDate\", \"Ticker\"]), on=[\"BarDate\", \"Ticker\"])\n",
    "    # Creates dataframe which will be filled with all classifications\n",
    "    df_predictions_lstm = df_y_test_lstm.copy()\n",
    "    # Join the column NextdayReturn from df to df_classifications_lstm by BarDate and Ticker\n",
    "    df_predictions_lstm = df_predictions_lstm.join(df.loc[:, df.columns.isin([\"BarDate\", \"Ticker\", \"NextdayReturn\"])].set_index([\"BarDate\", \"Ticker\"]), on=[\"BarDate\", \"Ticker\"])\n",
    "elif str_nn_type == \"SimpleRNN\":\n",
    "    # Creates dataframe which will be filled with all predictions\n",
    "    df_classifications_rnn = df_y_test_lstm.copy()\n",
    "    # Join the column NextdayReturn from df to df_classifications_lstm by BarDate and Ticker\n",
    "    df_classifications_rnn = df_classifications_rnn.join(df.loc[:, df.columns.isin([\"BarDate\", \"Ticker\", \"NextdayReturn\"])].set_index([\"BarDate\", \"Ticker\"]), on=[\"BarDate\", \"Ticker\"])\n",
    "    # Creates dataframe which will be filled with all classifications\n",
    "    df_predictions_rnn = df_y_test_lstm.copy()\n",
    "    # Join the column NextdayReturn from df to df_classifications_lstm by BarDate and Ticker\n",
    "    df_predictions_rnn = df_predictions_rnn.join(df.loc[:, df.columns.isin([\"BarDate\", \"Ticker\", \"NextdayReturn\"])].set_index([\"BarDate\", \"Ticker\"]), on=[\"BarDate\", \"Ticker\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 160 minutes to run\n",
    "# Takes #Epochs times 5 times 70sec to run\n",
    "\n",
    "import LSTM\n",
    "importlib.reload(LSTM)\n",
    "\n",
    "if str_nn_type == \"LSTM\":\n",
    "    ar_predictions_lstm, ar_classifications_lstm, model_lstm = LSTM.LSTM_test(best_model_LSTM, window_size, y_in_sample_lstm, X_in_sample_lstm, y_test_lstm, X_test_lstm, b_sentiment_score, n_past_returns, loss_function, str_nn_type) \n",
    "    df_predictions_lstm[\"predictions_lstm\"] = ar_predictions_lstm\n",
    "    df_classifications_lstm[\"classifications_lstm\"] = ar_classifications_lstm\n",
    "elif str_nn_type == \"SimpleRNN\":\n",
    "    ar_predictions_rnn, ar_classifications_rnn, model_rnn = LSTM.LSTM_test(best_model_RNN, window_size, y_in_sample_lstm, X_in_sample_lstm, y_test_lstm, X_test_lstm, b_sentiment_score, n_past_returns, loss_function, str_nn_type) \n",
    "    df_predictions_rnn[\"predictions_rnn\"] = ar_predictions_rnn\n",
    "    df_classifications_rnn[\"classifications_rnn\"] = ar_classifications_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joins LSTM predictions and classifications to df_predictions and df_classifications\n",
    "\n",
    "if str_nn_type == \"LSTM\":\n",
    "    # left join column classifications_lstm from df_classifications_lstm to df_classifications by BarDate and Ticker\n",
    "    df_classifications = df_classifications.join(df_classifications_lstm.loc[:, df_classifications_lstm.columns.isin([\"BarDate\", \"Ticker\", \"classifications_lstm\"])].set_index([\"BarDate\", \"Ticker\"]), on=[\"BarDate\", \"Ticker\"])\n",
    "    df_classifications = df_classifications.dropna()\n",
    "    del df_classifications_lstm\n",
    "    # left join column classifications_lstm from df_classifications_lstm to df_classifications by BarDate and Ticker\n",
    "    df_predictions = df_predictions.join(df_predictions_lstm.loc[:, df_predictions_lstm.columns.isin([\"BarDate\", \"Ticker\", \"predictions_lstm\"])].set_index([\"BarDate\", \"Ticker\"]), on=[\"BarDate\", \"Ticker\"])\n",
    "    df_predictions = df_predictions.dropna()\n",
    "    del df_predictions_lstm\n",
    "elif str_nn_type == \"SimpleRNN\":\n",
    "    # left join column classifications_lstm from df_classifications_lstm to df_classifications by BarDate and Ticker\n",
    "    df_classifications = df_classifications.join(df_classifications_rnn.loc[:, df_classifications_rnn.columns.isin([\"BarDate\", \"Ticker\", \"classifications_rnn\"])].set_index([\"BarDate\", \"Ticker\"]), on=[\"BarDate\", \"Ticker\"])\n",
    "    df_classifications = df_classifications.dropna()\n",
    "    del df_classifications_rnn\n",
    "    # left join column classifications_lstm from df_classifications_lstm to df_classifications by BarDate and Ticker\n",
    "    df_predictions = df_predictions.join(df_predictions_rnn.loc[:, df_predictions_rnn.columns.isin([\"BarDate\", \"Ticker\", \"predictions_rnn\"])].set_index([\"BarDate\", \"Ticker\"]), on=[\"BarDate\", \"Ticker\"])\n",
    "    df_predictions = df_predictions.dropna()\n",
    "    del df_predictions_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Evaluation_metrics\n",
    "importlib.reload(Evaluation_metrics)\n",
    "\n",
    "if str_nn_type == \"LSTM\":\n",
    "    predictions_accuracy = Evaluation_metrics.prediction_metrics(df_classifications, str_model = \"lstm\", str_single_metric = \"Accuracy\")\n",
    "    df_accuracy[\"LSTM\"] = predictions_accuracy       # If the backtest is so good but the accuracy in the top 20 not so then something is going wrong here!!!!!!!!! This can be tested by checking the number of longs and shorts above the median in portfolio \n",
    "\n",
    "elif str_nn_type == \"SimpleRNN\":\n",
    "    predictions_accuracy = Evaluation_metrics.prediction_metrics(df_classifications, str_model = \"rnn\", str_single_metric = \"Accuracy\")\n",
    "    df_accuracy[\"RNN\"] = predictions_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only the model from the last iteraton of the expanding window!\n",
    "# If we do Shap values or something then this must be mentioned!!!!!!!!!!!!!!!!!!!!!!!\n",
    "if str_nn_type == \"LSTM\":\n",
    "    model_lstm.summary()\n",
    "elif str_nn_type == \"SimpleRNN\":\n",
    "    model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall numpy\n",
    "# %pip install numpy==1.19.5\n",
    "\n",
    "# import shap\n",
    "\n",
    "# # We initialize the javascript for SHAP\n",
    "# shap.initjs()\n",
    "\n",
    "# X_train_lstm_sample = X_train_lstm[np.random.choice(X_train_lstm.shape[0], 1000, replace=False)]\n",
    "\n",
    "# # We create an explainer. DeepExplainer works well for deep learning models like LSTM.\n",
    "# explainer = shap.DeepExplainer(model_lstm, X_train_lstm_sample)\n",
    "\n",
    "# # Generate SHAP values\n",
    "# shap_values = explainer.shap_values(X_train_lstm_sample)\n",
    "\n",
    "# # Convert the input training set to a numpy array\n",
    "# X_train_array = np.array(X_train_lstm_sample)\n",
    "\n",
    "# # Summarize the effects of all the features\n",
    "# shap.summary_plot(shap_values, X_train_array)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification & Prediction analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_classifications[\"Target\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [\"classifications_lr\",  \"classifications_rf\",  \"classifications_gbc\", \"classifications_rnn\",  \"classifications_lstm\"]:\n",
    "    print(df_classifications[model].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts per quantile\n",
    "\n",
    "df_classifications_predictions = df_classifications.copy()\n",
    "# left join columns predictions_lr, predictions_rf, predictions_gbc, predictions_lstm from df_predictions to df_classifications_temp by BarDate and Ticker\n",
    "df_classifications_predictions = df_classifications_predictions.join(df_predictions.loc[:, df_predictions.columns.isin([\"BarDate\", \"Ticker\", \"predictions_lr\", \"predictions_rf\", \n",
    "                                                                                                                        \"predictions_gbc\", \"predictions_rnn\", \"predictions_lstm\"])].set_index([\"BarDate\", \"Ticker\"]), on=[\"BarDate\", \"Ticker\"])\n",
    "\n",
    "models = [\"lr\", \"rf\", \"gbc\", \"rnn\", \"lstm\"]\n",
    "dict_quantiles = {}\n",
    "for model in models:\n",
    "\n",
    "    # Group by date\n",
    "    grouped = df_classifications_predictions[[\"BarDate\", \"Ticker\", \"Target\", f\"classifications_{model}\", f\"predictions_{model}\"]].groupby(\"BarDate\")\n",
    "\n",
    "    # Select top 20, reset index, calculate str_single_metric of quantile\n",
    "    quantile_top = grouped.apply(lambda x: x.sort_values(by=f\"predictions_{model}\", \n",
    "                                                        ascending=False).iloc[: 20])\n",
    "    quantile_top = quantile_top.reset_index(drop=True)\n",
    "    # Select top 20, reset index, calculate str_single_metric of quantile\n",
    "    quantile_bottom = grouped.apply(lambda x: x.sort_values(by=f\"predictions_{model}\", \n",
    "                                                        ascending=False).iloc[int(len(x))-20 : ])\n",
    "    quantile_bottom = quantile_bottom.reset_index(drop=True)\n",
    "    quantile = pd.concat([quantile_top, quantile_bottom])\n",
    "    dict_quantiles[f\"quantile_{model}\"] = quantile\n",
    "\n",
    "    # Count the number of 0's and 1's for column classifications_lr in quantile\n",
    "    print(dict_quantiles[f\"quantile_{model}\"][f\"classifications_{model}\"].value_counts())\n",
    "\n",
    "    dict_quantiles[f\"quantile_{model}\"][f\"error_{model}\"] = np.where(dict_quantiles[f\"quantile_{model}\"][\"Target\"] == dict_quantiles[f\"quantile_{model}\"][f\"classifications_{model}\"], 0, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diebold Mariano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DM tests!!!!!!!!!\n",
    "import Evaluation_metrics\n",
    "importlib.reload(Evaluation_metrics)\n",
    "\n",
    "str_dm_type = \"adjusted\"   # \"adjusted\" or \"classic\"\n",
    "str_sample = \"Top_bottom_20\"        # \"full\" or \"Top_bottom_20\"\n",
    "# Do not use classic in combination with Top_bottom_20!!!!!!!!!!!!! Not possible\n",
    "dmTable, pValueTable = Evaluation_metrics.Diebold_Mariano(df_classifications, df_predictions, str_dm_type, str_sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio backtests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Portfolio part code needs to be adjusted maybe, seminar model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write code here to load gb, lr, rf predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_only = True\n",
    "# short_only = False\n",
    "\n",
    "# # Number of long and short stocks\n",
    "# n_both = 20\n",
    "# n_long = n_both\n",
    "# n_short = n_both\n",
    "\n",
    "# Choose which models we want\n",
    "backtest_models = [\"lr\", \"rf\", \"gbc\", \"lstm\"]  #[\"lr\", \"rf\", \"gbc\", \"rnn\", \"lstm\"]                          #\"NN1\", \"NN2\", \"NN3\", \"NN4\", \"NN5\"] \n",
    "legend_names = [\"LR\", \"RF\", \"GB\", \"LSTM\"]         #[\"LR\", \"RF\", \"GB\", \"RNN\", \"LSTM\"]\n",
    "\n",
    "# Choose weighting method (Equal weighted (Equal) or RankBased weighted (RankBased))\n",
    "# weighting_method = \"Equal\" \n",
    "\n",
    "dict_df_returns_portfolio = {}\n",
    "dict_df_metrics = {}\t\n",
    "\n",
    "# # Cumulative return of stock index, used only in plot\n",
    "# stockindex_returns_cum = (1 + df_stockindex_returns[\"Return\"]).cumprod() - 1\n",
    "df_stockindex_returns = None\n",
    "\n",
    "# Define the colors for the lines\n",
    "# colors = ['red', 'blue', 'green', 'orange', 'red', 'blue', 'green', 'orange']\n",
    "colors = ['red', 'blue', 'green', 'orange', 'yellow']\n",
    "\n",
    "# Get factors and rf rate\n",
    "rf_rate_and_factors = pd.read_csv(os.path.join(r\"C:\\Users\\BasPeeters\\OneDrive - FactorOrange.capital\\Master Thesis\\Dataframes and output\", \"RF_rate_and_factors.csv\"))\n",
    "rf_rate_and_factors = rf_rate_and_factors['BarDate;Mkt-RF;SMB;HML;RMW;CMA;RF'].str.split(';', expand=True)\n",
    "rf_rate_and_factors.columns = ['BarDate', 'Mkt_min_RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']\n",
    "rf_rate_and_factors['BarDate'] = pd.to_datetime(rf_rate_and_factors['BarDate'], format='%d/%m/%Y')\n",
    "# Select all rows from rf_rate_and_factors where BarDate is in df_predictions\n",
    "rf_rate_and_factors = rf_rate_and_factors[rf_rate_and_factors['BarDate'].isin(df_predictions['BarDate'])]\n",
    "rf_rate_and_factors.reset_index(drop=True, inplace=True)\n",
    "# change dtype of rf_rate_and_factors to float\n",
    "rf_rate_and_factors[[\"Mkt_min_RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\", \"RF\"]] = rf_rate_and_factors[[\"Mkt_min_RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\", \"RF\"]].astype('float32')\n",
    "rf_rate_and_factors[['Mkt_min_RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']] = rf_rate_and_factors[['Mkt_min_RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 5:30 to run\n",
    "\n",
    "# # Creates a matrix with all actual returns only \n",
    "# # The last columns are not the last dates. These all seem NaN values.............Something must change...........................\n",
    "# stocks = df['Ticker'].unique()\n",
    "# returnsMatrix = pd.DataFrame(index=df_predictions['BarDate'].unique(), columns=stocks)\n",
    "# for stock in stocks:\n",
    "#     stock_df = df[df['Ticker'] == stock]\n",
    "#     for index, row in stock_df.iterrows():\n",
    "#         returnsMatrix.at[row['BarDate'], stock] = row['NextdayReturn']\n",
    "\n",
    "# Load returnsMatrix\n",
    "returnsMatrix = pd.read_csv(os.path.join(r\"C:\\Users\\BasPeeters\\OneDrive - FactorOrange.capital\\Master Thesis\\Dataframes and output\", \"returnsMatrix.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### later nog metrics opslaan voor long_only and short_only maar pas bij de backtests die we echt gaan gebruiken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "# Extra metrics take long to run\n",
    "# +- 72min for full run with 10,20,50 and all models\n",
    "\n",
    "import Portfolio_backtester\n",
    "importlib.reload(Portfolio_backtester)\n",
    "\n",
    "# Number of long and short stocks\n",
    "for n_both in tqdm([20]):                   #([10, 20, 50]):\n",
    "\n",
    "    #n_both = 20\n",
    "    n_long = n_both\n",
    "    n_short = n_both\n",
    "    n_constituents = n_both * 2\n",
    "    # Create a loop which first sets long_only = True and then short_only = True and then both = False\n",
    "    for long_only in tqdm([True, False]):\n",
    "        for short_only in [True, False]:\n",
    "            if (long_only == True) & (short_only == True):\n",
    "                continue\n",
    "            if long_only == True:\n",
    "                str_side = \"long\"\n",
    "            if short_only == True:\n",
    "                str_side = \"short\"\n",
    "            if (long_only == False) & (short_only == False):\n",
    "                str_side = \"long_short\"\n",
    "\n",
    "            # Plot the cumulative portfolio return\n",
    "            plt.subplots(figsize=(20,10))\n",
    "            # Dictionary to be filled with weight matrices\n",
    "            weight_matrix_dict = {}\n",
    "            # Use this for color of the lines\n",
    "            i = 0\n",
    "            # for weighting_method in tqdm([\"Equal\", \"RankBased\"]):\n",
    "            for weighting_method in [\"Equal\"]:\n",
    "                # dict_df_metrics[f\"{weighting_method}_{n_both}_{str_side}\"] = pd.DataFrame({}, index=[\"mean_return\", 'std_dev', 'information_ratio', \"sharpe_ratio\", \"appraisal_ratio\", 'max_drawdown', 'max_1_day_loss', \"turnover\"])\n",
    "                dict_df_returns_portfolio[f\"{weighting_method}_{n_both}_{str_side}\"] = pd.DataFrame({\"BarDate\": df_predictions[\"BarDate\"].unique()})\n",
    "                for backtest_model in tqdm(backtest_models):\n",
    "                    # Predictions are now stored in two different objects\n",
    "                    # if backtest_model == \"lstm\":\n",
    "                    #     df_predictions_backtest = df_predictions_lstm\n",
    "                    #     df_returns_portfolio_backtest = df_returns_portfolio_lstm\n",
    "\n",
    "                    predictions_test_set, df_returns_portfolio, str_returns_portfolio_cum, str_logreturns_portfolio_cum, weightMatrix = Portfolio_backtester.backtest(df_predictions,n_long,n_short,backtest_model,\n",
    "                                                                                    weighting_method, dict_df_returns_portfolio[f\"{weighting_method}_{n_both}_{str_side}\"], returnsMatrix, long_only, short_only)\n",
    "                    weight_matrix_dict.update({backtest_model: weightMatrix})     # Weight matrix does not seem correct!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            \n",
    "                    if weighting_method == \"Equal\":\n",
    "                        plt.plot(pd.to_datetime(df_returns_portfolio[\"BarDate\"]), df_returns_portfolio[str_logreturns_portfolio_cum], color = colors[i], label=str(f\"{backtest_model} {weighting_method}\"))\n",
    "                    elif(weighting_method == \"RankBased\"):\n",
    "                        plt.plot(pd.to_datetime(df_returns_portfolio[\"BarDate\"]), df_returns_portfolio[str_logreturns_portfolio_cum], color = colors[i], linestyle = '--', label=str(f\"{backtest_model} {weighting_method}\"))\n",
    "                    # Adds portfolio returns to the dictionary with all portfolio returns\n",
    "                    dict_df_returns_portfolio[f\"{weighting_method}_{n_both}_{str_side}\"] = df_returns_portfolio\n",
    "                    # Gives metrics, now only for the full long-short portfolio\n",
    "                    # if long_only == True or short_only == True:\n",
    "                    # dict_df_metrics[f\"{weighting_method}_{n_both}_{str_side}\"] = Portfolio_backtester.metrics(dict_df_returns_portfolio[f\"{weighting_method}_{n_both}_{str_side}\"], backtest_model, dict_df_metrics[f\"{weighting_method}_{n_both}_{str_side}\"], df_stockindex_returns, returnsMatrix, weightMatrix, rf_rate_and_factors)\n",
    "                    i = i + 1\n",
    "\n",
    "            #plt.plot(pd.to_datetime(df_stockindex_returns[\"Date\"]), stockindex_returns_cum) # Plots long only index\n",
    "            plt.axvspan(datetime.datetime.strptime(\"29/02/2020\", '%d/%m/%Y').date(), datetime.datetime.strptime(\"31/03/2020\", '%d/%m/%Y').date(), color=\"grey\", alpha=0.5)\n",
    "            # plt.axvspan(datetime.datetime.strptime(\"31/12/2007\", '%d/%m/%Y').date(), datetime.datetime.strptime(\"31/05/2009\", '%d/%m/%Y').date(), color=\"grey\", alpha=0.5)\n",
    "\n",
    "            plt.xlabel(xlabel=\"Date\", fontsize = \"x-large\")\n",
    "            if long_only == True:\n",
    "                plt.ylabel(ylabel=\"Cumulative (long) logreturns\", fontsize = \"x-large\")\n",
    "            if short_only == True:\n",
    "                plt.ylabel(ylabel=\"Cumulative (short) logreturns\", fontsize = \"x-large\")\n",
    "            if (long_only == False) & (short_only == False):\n",
    "                plt.ylabel(ylabel=\"Cumulative logreturns\", fontsize = \"x-large\")\n",
    "            plt.title(label=f\"Cumulative {n_constituents}-stock portfolio logreturns\", fontsize = \"xx-large\")  \n",
    "            plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "            #plt.legend(['lr', 'rf', 'gb', 'lstm', 'lr_rank', 'rf_rank', 'gb_rank', 'lstm_rank'] , loc = 'upper center', ncols = 4, fontsize = 'large')    # Without specifying loc it chooses the best location\n",
    "            plt.legend(legend_names  , loc = 'upper center', ncols = 4, fontsize = 'large')    # Without specifying loc it chooses the best location\n",
    "\n",
    "            # Format the x-axis to display years\n",
    "            years = mdates.YearLocator()   # every year\n",
    "            years_fmt = mdates.DateFormatter('%Y')\n",
    "            plt.gca().xaxis.set_major_locator(years)\n",
    "            plt.gca().xaxis.set_major_formatter(years_fmt)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only calculates metrics\n",
    "\n",
    "import Portfolio_backtester\n",
    "importlib.reload(Portfolio_backtester)\n",
    "\n",
    "dict_df_metrics = {}\n",
    "for n_both in [10, 20, 50]:\n",
    "    for long_only in [True, False]:\n",
    "        for short_only in [True, False]:\n",
    "            if (long_only == True) & (short_only == True):\n",
    "                continue\n",
    "            if long_only == True:\n",
    "                str_side = \"long\"\n",
    "            if short_only == True:\n",
    "                str_side = \"short\"\n",
    "            if (long_only == False) & (short_only == False):\n",
    "                str_side = \"long_short\"                \n",
    "            for weighting_method in [\"Equal\"]:\n",
    "                dict_df_metrics[f\"{weighting_method}_{n_both}_{str_side}\"] = pd.DataFrame({}, index=[\"mean_return\", 'std_dev', 'information_ratio', \"sharpe_ratio\", \"appraisal_ratio\", 'max_drawdown', 'max_1_day_loss', \"turnover\"])    \n",
    "                for backtest_model in (backtest_models):\n",
    "                    dict_df_metrics[f\"{weighting_method}_{n_both}_{str_side}\"] = Portfolio_backtester.metrics(dict_df_returns_portfolio[f\"{weighting_method}_{n_both}_{str_side}\"], backtest_model, dict_df_metrics[f\"{weighting_method}_{n_both}_{str_side}\"], df_stockindex_returns, returnsMatrix, weightMatrix, rf_rate_and_factors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_df_metrics[\"Equal_20_long_short\"])\n",
    "# Check the distribution of column \"returns_lr_portfolio\" in dict_df_returns_portfolio[\"Equal_20_long_short\"]\n",
    "print(dict_df_returns_portfolio[\"Equal_20_long_short\"][\"returns_lr_portfolio\"].hist(bins=100))\n",
    "# Check the distribution of column \"returns_lr_portfolio\" in dict_df_returns_portfolio[\"Equal_20_long_short\"]\n",
    "# print(dict_df_returns_portfolio[\"Equal_20_long_short\"][\"returns_lr_portfolio\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of rows for each year of the dataframe\n",
    "print(\"Trading days per year\")\n",
    "dict_df_returns_portfolio[\"Equal_20_long_short\"].groupby(dict_df_returns_portfolio[\"Equal_20_long_short\"][\"BarDate\"].dt.year).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change for new file!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "b_sentiment_score \n",
    "LSTM_model_type = \"LSTM_model_1\"\n",
    "time_steps\n",
    "# n_both\n",
    "# weighting_method\n",
    "n_nodes = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Creating the directory name\n",
    "directory = r\"C:\\Users\\BasPeeters\\OneDrive - FactorOrange.capital\\Master Thesis\\Dataframes and output\"\n",
    "folder_name = f\"SESI={b_sentiment_score}_{LSTM_model_type}_time_steps={time_steps}_n_nodes={n_nodes}\"\n",
    "full_directory = os.path.join(directory, folder_name)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(full_directory):\n",
    "    os.makedirs(full_directory)\n",
    "\n",
    "# # Save predicitons and classifications\n",
    "# try:\n",
    "#     df_predictions.to_pickle(os.path.join(full_directory, \"df_predictions.pkl\"))\n",
    "# except:\n",
    "#     print(\"df_predictions does not exist\")\n",
    "# try:\n",
    "#     df_classifications.to_pickle(os.path.join(full_directory, \"df_classifications.pkl\"))\n",
    "# except:\n",
    "#     print(\"df_classifications does not exist\")\n",
    "\n",
    "# # Save accuracy \n",
    "# df_accuracy.to_csv(os.path.join(full_directory, \"df_accuracy.csv\"))\n",
    "\n",
    "# # Save model parameters\n",
    "# try: \n",
    "#     dump(best_model_LSTM, os.path.join(full_directory, 'best_model_LSTM.joblib'))\n",
    "# except:\n",
    "#     print(\"best_model_LSTM does not exist\")\t\n",
    "# try:\n",
    "#     dump(best_model_GBC, os.path.join(full_directory, 'best_model_GBC.joblib'))\n",
    "# except:\n",
    "#     print(\"best_model_GBC does not exist\")\n",
    "# try:\n",
    "#     dump(best_model_RF, os.path.join(full_directory, 'best_model_RF.joblib'))\n",
    "# except:\n",
    "#     print(\"best_model_RF does not exist\")\n",
    "# try:\n",
    "#     dump(best_model_RNN, os.path.join(full_directory, 'best_model_RNN.joblib'))\n",
    "# except:\n",
    "#     print(\"best_model_RNN does not exist\")\n",
    "\n",
    "\n",
    "# # Save Actual lstm model\n",
    "# try:\n",
    "#     model_name = f\"SESI={b_sentiment_score}_{LSTM_model_type}_time_steps={time_steps}_n_nodes={n_nodes}\"\n",
    "#     model_lstm.save(os.path.join(full_directory,f\"{model_name}.h5\"))\n",
    "# except:\n",
    "#     print(\"model_lstm does not exist\")\n",
    "\n",
    "# Portfolio metrics\n",
    "try:\n",
    "    for key in dict_df_metrics:\n",
    "        dict_df_metrics[key].to_csv(os.path.join(full_directory, f'df_metrics_{key}.csv'))\n",
    "except:\n",
    "    print(\"dict_df_metrics does not exist\")\n",
    "\n",
    "try:\n",
    "    for key in dict_df_returns_portfolio:\n",
    "        dict_df_returns_portfolio[key].to_pickle(os.path.join(full_directory, f\"df_returns_portfolio_{key}.pkl\"))\n",
    "except:\n",
    "    print(\"dict_df_returns_portfolio does not exist\")\n",
    "\n",
    "print(f\"All data is saved to {full_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM sequences\n",
    "\n",
    "# Creating the directory name\n",
    "directory = r\"C:\\Users\\BasPeeters\\OneDrive - FactorOrange.capital\\Master Thesis\\Dataframes and output\\lstm_sequences\"\n",
    "folder_name = f\"SESI={b_sentiment_score}_time_steps={time_steps}\"\n",
    "full_directory = os.path.join(directory, folder_name)\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(full_directory):\n",
    "    os.makedirs(full_directory)\n",
    "\n",
    "variables = [X_in_sample_lstm, X_test_lstm, y_in_sample_lstm, y_test_lstm, X_train_lstm, X_val_lstm, y_train_lstm, y_val_lstm, df_y_test_lstm]\n",
    "names = ['X_in_sample_lstm', 'X_test_lstm', 'y_in_sample_lstm', 'y_test_lstm', 'X_train_lstm', 'X_val_lstm', 'y_train_lstm', 'y_val_lstm', \"df_y_test_lstm\"]\n",
    "\n",
    "for var, name in zip(variables, names):\n",
    "    with open(os.path.join(full_directory, f\"{name}.pkl\"), 'wb') as f:\n",
    "        pickle.dump(var, f)\n",
    "del variables, names, var, name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change for new file!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "b_sentiment_score = False\n",
    "LSTM_model_type = \"LSTM_model_1\"\n",
    "time_steps = 3\n",
    "# n_both\n",
    "# weighting_method\n",
    "n_nodes = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "# Creating the directory name\n",
    "directory = r\"C:\\Users\\BasPeeters\\OneDrive - FactorOrange.capital\\Master Thesis\\Dataframes and output\"\n",
    "folder_name = f\"SESI={b_sentiment_score}_{LSTM_model_type}_time_steps={time_steps}_n_nodes={n_nodes}\"\n",
    "full_directory = os.path.join(directory, folder_name)\n",
    "\n",
    "df_predictions = pd.read_pickle(os.path.join(full_directory, \"df_predictions.pkl\"))\n",
    "df_classifications = pd.read_pickle(os.path.join(full_directory, \"df_classifications.pkl\"))\n",
    "df_accuracy = pd.read_csv(os.path.join(full_directory, \"df_accuracy.csv\"))\n",
    "\n",
    "# dict_df_metrics = {}\n",
    "# for key in dict_df_metrics:\n",
    "#     dict_df_metrics[key] = pd.read_csv(os.path.join(full_directory, f'df_metrics_{key}.csv'))\n",
    "\n",
    "# for key in dict_df_returns_portfolio:\n",
    "#     dict_df_returns_portfolio[key] = pd.read_pickle(os.path.join(full_directory, f\"df_returns_portfolio_{key}.pkl\"))\n",
    "\n",
    "# best_model_LSTM = load(os.path.join(full_directory, \"best_model_LSTM.joblib\"))\n",
    "# best_model_GBC = load(os.path.join(full_directory, \"best_model_GBC.joblib\"))\n",
    "# best_model_RF = load(os.path.join(full_directory, \"best_model_RF.joblib\"))\n",
    "# best_model_RNN = load(os.path.join(full_directory, \"best_model_RNN.joblib\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have your data in the following format:\n",
    "# data = {\n",
    "#     'Portfolio1': {'10': {'Accuracy': 0.8, 'Sharpe Ratio': 1.2, 'Std Dev': 0.1, 'Return': 0.05},\n",
    "#                    '20': {'Accuracy': 0.75, 'Sharpe Ratio': 1.1, 'Std Dev': 0.15, 'Return': 0.06},\n",
    "#                    '30': {'Accuracy': 0.85, 'Sharpe Ratio': 1.3, 'Std Dev': 0.12, 'Return': 0.07}},\n",
    "#     'Portfolio2': {'10': {'Accuracy': 0.9, 'Sharpe Ratio': 1.4, 'Std Dev': 0.11, 'Return': 0.08},\n",
    "#                    '20': {'Accuracy': 0.88, 'Sharpe Ratio': 1.5, 'Std Dev': 0.13, 'Return': 0.09},\n",
    "#                    '30': {'Accuracy': 0.92, 'Sharpe Ratio': 1.6, 'Std Dev': 0.14, 'Return': 0.1}},\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "data = {\n",
    "    'Portfolio1': {'10': {'Accuracy': 0.8, 'Sharpe Ratio': 1.2, 'Std Dev': 0.1, 'Return': 0.05},\n",
    "                   '20': {'Accuracy': 0.75, 'Sharpe Ratio': 1.1, 'Std Dev': 0.15, 'Return': 0.06},\n",
    "                   '30': {'Accuracy': 0.85, 'Sharpe Ratio': 1.3, 'Std Dev': 0.12, 'Return': 0.07}},\n",
    "    'Portfolio2': {'10': {'Accuracy': 0.9, 'Sharpe Ratio': 1.4, 'Std Dev': 0.11, 'Return': 0.08},\n",
    "                   '20': {'Accuracy': 0.88, 'Sharpe Ratio': 1.5, 'Std Dev': 0.13, 'Return': 0.09},\n",
    "                   '30': {'Accuracy': 0.92, 'Sharpe Ratio': 1.6, 'Std Dev': 0.14, 'Return': 0.1}},\n",
    "    # Add more portfolios as needed\n",
    "}\n",
    "\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']  # Add more colors if you have more than 7 portfolios\n",
    "\n",
    "fig, ax = plt.subplots(4, 3, figsize=(10, 10))\n",
    "\n",
    "# Plotting\n",
    "for i, metric in enumerate(['Accuracy', 'Sharpe Ratio', 'Std Dev', 'Return']):\n",
    "    for j, num_stocks in enumerate(['10', '20', '30']):\n",
    "        ax[i, j].bar(data.keys(), [v[num_stocks][metric] for v in data.values()], color=colors)\n",
    "        if i == 0:\n",
    "            ax[i, j].set_title(f'{num_stocks} stocks')\n",
    "        if j == 0:\n",
    "            ax[i, j].set_ylabel(metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
