{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import importlib\n",
    "from warnings import simplefilter\n",
    "from six import StringIO\n",
    "from IPython.display import Image\n",
    "\n",
    "# Reload external files\n",
    "import importlib\n",
    "\n",
    "# Mute warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change function of EOD because we use Compustat....\n",
    "# In the end, add all used Excel files to this repo, we have: stock_universe_new.joblib, SP500H_EOD_raw, SP500H_RP_SESI (if we only use daily SESI score and no other sentiment feature types)\n",
    "# We use EOD for the list of historical components\n",
    "# At the end, just do all data steps one more time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose data location, EOD, SQL or CSV, Compustat\n",
    "str_load_data = \"CSV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This entire notebook should go in a separate ipynb file later\n",
    "\n",
    "# Loads raw data, data cleaning and transformation is needed after this\n",
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "\n",
    "if str_load_data == \"EOD_old_method\":\n",
    "    \n",
    "    # Index name \n",
    "    str_index_name = \"S&P500_historical\"\n",
    "\n",
    "    # Loads market data from EOD\n",
    "    ar_index, df_index = Load_data.create_stock_universe_old(str_index_name)\n",
    "    df = Load_data.load_EOD(ar_index)\n",
    "\n",
    "    df = Load_data.create_historical_SP_Index_old_method(df_index, df)            \n",
    "\n",
    "    # Adds lags and returns\n",
    "    df = Load_data.add_lags(df)\n",
    "\n",
    "if str_load_data == \"EOD_new_method\":\n",
    "\n",
    "    df = Load_data.create_historical_SP_Index()\n",
    "\n",
    "    # Adds lags and returns\n",
    "    df = Load_data.add_lags(df)\n",
    "\n",
    "elif str_load_data == \"SQL\":\n",
    "    \n",
    "    # Name of the dataframe\n",
    "    str_table_name = \"SP500H_RP_raw\"\n",
    "\n",
    "    # Specify column subset, None if all columns should be loaded\n",
    "    str_column_subset = \"[TIMESTAMP_TZ], [RP_ENTITY_ID], [ENTITY_NAME], [EVENT_SENTIMENT], [EVENT_RELEVANCE], [EVENT_SIMILARITY_DAYS]\"\n",
    "\n",
    "    # Loads the dataframe from SQL\n",
    "    df = Load_data.load_SQL(str_table_name, str_column_subset)\n",
    "\n",
    "elif str_load_data == \"CSV\":\n",
    "\n",
    "    df_full = Load_data.load_csv(\"df_Compustat_SPH\")\n",
    "\n",
    "# Check the number of unique elements of SymbolExchangeCode for every BarDate\n",
    "# df_test = df.groupby(\"BarDate\")[\"Ticker\"].nunique()\n",
    "\n",
    "# Plot the number of unique elements of SymbolExchangeCode for every BarDate\n",
    "# df.groupby(\"BarDate\")[\"Ticker\"].nunique().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all possible places to df when ready....... (to save space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "# Loads raw Compustat data from CSV\n",
    "df_Compustat_raw = Load_data.load_csv(\"df_Compustat_raw\")\n",
    "df_Compustat_raw.groupby(\"BarDate\")[\"Ticker\"].nunique().plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Historical SP500 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "df_Compustat_raw_SPH = Load_data.create_historical_SP_Index_deliver(df_Compustat_raw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add lags, returns and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "df_Compustat_raw_SPH_variables = Load_data.add_variables(df_Compustat_raw_SPH)\n",
    "df_Compustat_raw_SPH_variables.groupby(\"BarDate\")[\"Ticker\"].nunique().plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "# Loads raw RavenPack data from CSV\n",
    "df_RP_raw = Load_data.load_csv(\"df_RP_raw\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SESI score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SESI\n",
    "importlib.reload(SESI)\n",
    "# Creates daily SESI score from raw RavenPack data\n",
    "df_SESI = SESI.SESI(df_RP_raw)\n",
    "# Adds Ticker column to df_SESI\n",
    "df_SESI = SESI.add_ticker(df_SESI)\n",
    "df_SESI[\"TIMESTAMP_TZ\"] = pd.to_datetime(df_SESI[\"TIMESTAMP_TZ\"])\n",
    "df_SESI.groupby(\"TIMESTAMP_TZ\")[\"Ticker\"].nunique().plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge df_SESI and df_market to df and add lagged SESI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes long to run, +-1100min\n",
    "# Was very quick with new function (only 3sec)\n",
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "df = Load_data.add_SESI(df_Compustat_raw_SPH_variables, df_SESI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load or Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "file_name = \"df\"\n",
    "df = Load_data.load_csv(file_name)\n",
    "\n",
    "# import Save_data\n",
    "# importlib.reload(Save_data)\n",
    "# file_name = \"df_Compustat_raw_SPH\"\n",
    "# Save_data.save_to_csv(df_Compustat_raw_SPH, file_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Analyze_data\n",
    "importlib.reload(Analyze_data)\n",
    "\n",
    "# Check the number of unique elements of SymbolExchangeCode for every BarDate\n",
    "Analyze_data.unique_stocks_by_date(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Analyze_data\n",
    "importlib.reload(Analyze_data)\n",
    "\n",
    "# Plot all stocks\n",
    "Analyze_data.plot_all_stocks(df, max_AdjustedClose = 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show summary statistics for CS_MedianNextDayReturn\n",
    "print(df[[\"PreviousdayReturn\", \"SESI\"]].describe())\n",
    "temp = df[[\"PreviousdayReturn\", \"SESI\"]].describe()\n",
    "temp.to_csv(\"Descriptive_statistics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count all 0 values in SESI\n",
    "df[df[\"SESI\"] == 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Descriptive Statistics\n",
    "# Get Descriptive Statistics with 3 decimal places\n",
    "desc_stats_PDR = df['PreviousdayReturn'].describe().apply(lambda x: format(x, '.3f'))\n",
    "desc_stats_SESI = df['SESI'].describe().apply(lambda x: format(x, '.3f'))\n",
    "\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, axes = plt.subplots(1, 2, figsize=(10, 5), sharex=False)\n",
    "\n",
    "\n",
    "# Plot a simple histogram with binsize determined automatically\n",
    "sns.histplot(df['PreviousdayReturn'], color=\"b\", ax=axes[0])\n",
    "axes[0].set_xlabel('Returns', fontsize=\"x-large\")\n",
    "axes[0].set_ylabel('Count', fontsize=\"x-large\")\n",
    "# axes[0].text(x=0.97, y=0.97, transform=axes[0].transAxes, s=desc_stats_PDR.to_string(), \n",
    "#               fontweight='demibold', fontsize=15, verticalalignment='top', horizontalalignment='right', \n",
    "#               backgroundcolor='white', color='black')\n",
    "plt.xticks(fontsize=\"large\")\n",
    "plt.yticks(fontsize=\"large\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Draw a table of descriptive statistics for PreviousdayReturn\n",
    "# desc_table_PDR = axes[0].table(cellText=list(map(list, zip(*[desc_stats_PDR.index, desc_stats_PDR.values]))),\n",
    "#                            colWidths=[0.2, 0.1],\n",
    "#                            cellLoc = 'center', loc='right')\n",
    "# desc_table_PDR.auto_set_font_size(False)\n",
    "# desc_table_PDR.set_fontsize(10)\n",
    "# desc_table_PDR.scale(0.6, 1.5) # you can adjust the size of the table here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot a kernel density estimate and rug plot\n",
    "sns.histplot(df['SESI'], color=\"b\", ax=axes[1],  bins=71)\n",
    "# axes[1].text(x=0.97, y=0.97, transform=axes[1].transAxes, s=desc_stats_SESI.to_string(), \n",
    "#               fontweight='demibold', fontsize=15, verticalalignment='top', horizontalalignment='right', \n",
    "#               backgroundcolor='white', color='black')\n",
    "axes[1].set_xlabel('SESI', fontsize=\"x-large\")\n",
    "axes[1].set_ylabel('Count', fontsize=\"x-large\")\n",
    "plt.xticks(fontsize=\"large\")\n",
    "plt.yticks(fontsize=\"large\")\n",
    "\n",
    "\n",
    "# # Draw a table of descriptive statistics for SESI\n",
    "# desc_table_SESI = axes[1].table(cellText=list(map(list, zip(*[desc_stats_SESI.index, desc_stats_SESI.values]))),\n",
    "#                             colWidths=[0.2, 0.1],\n",
    "#                             cellLoc = 'center', loc='right')\n",
    "# desc_table_SESI.auto_set_font_size(False)\n",
    "# desc_table_SESI.set_fontsize(10)\n",
    "# desc_table_SESI.scale(0.6, 1.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which metrics you want to check\n",
    "\n",
    "bool_dict = {\n",
    "    'descriptive_stats': True,\n",
    "    'correlation_matrix': False,\n",
    "    \"histogram\": False,\n",
    "    \"boxplot\": False,\n",
    "    \"scatterplot\": False,\n",
    "    \"lineplot\": False,\n",
    "    \"heatmap\": False,\n",
    "    \"barplot\": False,\n",
    "    \"piechart\": False,\n",
    "    \"violinplot\": False,\n",
    "    \"kdeplot\": False,\n",
    "    \"hexbinplot\": False,\n",
    "    \"scatter_matrix\": False,\n",
    "    \"parallel_coordinates\": False,\n",
    "    \"andrews_curves\": False,\n",
    "    \"radviz\": False,\n",
    "    \"lag_plot\": False,\n",
    "    \"autocorrelation_plot\": False,\n",
    "    \"bootstrap_plot\": False\n",
    "}\n",
    "\n",
    "import Analyze_data\n",
    "importlib.reload(Analyze_data)\n",
    "\n",
    "Analyze_data.Data_analysis(df, bool_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import importlib\n",
    "from warnings import simplefilter\n",
    "from six import StringIO\n",
    "from IPython.display import Image\n",
    "\n",
    "# Reload external files\n",
    "import importlib\n",
    "\n",
    "# Mute warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Load_data\n",
    "importlib.reload(Load_data)\n",
    "file_name = \"df\"\n",
    "df = Load_data.load_csv(file_name)\n",
    "df[\"BarDate\"] = pd.to_datetime(df[\"BarDate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[f'SESI_lead'] = df.groupby('Ticker')['SESI'].shift(-1)\n",
    "df[df[\"Ticker\"] == \"AAPL\"].head(10) \n",
    "# Count NaN values in df\n",
    "print(df.isna().sum())\n",
    "df = df.dropna()\n",
    "df[\"SESI\"] = df[\"SESI_lead\"]\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Order df by BarDate and SymbolExchangeCode\n",
    "# df = df.sort_values(by = [\"BarDate\", \"Ticker\"])\n",
    "# # reset index\n",
    "# df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 'Hyperparameter' is the sequence_length/time_steps\n",
    "time_steps = 3\n",
    "b_sentiment_score = True\n",
    "n_past_returns = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of 1's and 0's in column NextDayDirection\n",
    "df[\"NextDayDirection\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "avg_return = df.groupby('BarDate')['NextdayReturn'].mean()\n",
    "# Calculate the log cumulative return\n",
    "log_cumulative_return = np.log(1 + avg_return).cumsum()\n",
    "\n",
    "# Print and plot the result\n",
    "print(log_cumulative_return)\n",
    "\n",
    "plt.subplots(figsize=(15,6))\n",
    "\n",
    "plt.plot(log_cumulative_return)\n",
    "\n",
    "\n",
    "plt.axvspan(datetime.datetime.strptime(\"01/03/2001\", '%d/%m/%Y').date(), datetime.datetime.strptime(\"30/11/2001\", '%d/%m/%Y').date(), color=\"grey\", alpha=0.5)\n",
    "plt.axvspan(datetime.datetime.strptime(\"29/02/2020\", '%d/%m/%Y').date(), datetime.datetime.strptime(\"31/03/2020\", '%d/%m/%Y').date(), color=\"grey\", alpha=0.5)\n",
    "plt.axvspan(datetime.datetime.strptime(\"31/12/2007\", '%d/%m/%Y').date(), datetime.datetime.strptime(\"31/05/2009\", '%d/%m/%Y').date(), color=\"grey\", alpha=0.5)\n",
    "\n",
    "\n",
    "# plt.axvspan(datetime.datetime.strptime(\"15/06/2000\", '%d/%m/%Y').date(), datetime.datetime.strptime(\"21/01/2010\", '%d/%m/%Y').date(), color=\"red\", alpha=0.05, label=\"Training\")\n",
    "# plt.axvspan(datetime.datetime.strptime(\"22/01/2010\", '%d/%m/%Y').date(), datetime.datetime.strptime(\"04/05/2012\", '%d/%m/%Y').date(), color=\"blue\", alpha=0.05, label=\"Validation\")\n",
    "# plt.axvspan(datetime.datetime.strptime(\"05/05/2012\", '%d/%m/%Y').date(), datetime.datetime.strptime(\"07/03/2023\", '%d/%m/%Y').date(), color=\"green\", alpha=0.05, label=\"Test\")\n",
    "\n",
    "plt.fill_between(log_cumulative_return.index, log_cumulative_return.min(), log_cumulative_return, where=log_cumulative_return.index < datetime.datetime.strptime(\"21/01/2010\", '%d/%m/%Y'), color=\"red\", alpha=0.05, label=\"Training\")\n",
    "plt.fill_between(log_cumulative_return.index, log_cumulative_return.min(), log_cumulative_return, where=(log_cumulative_return.index >= datetime.datetime.strptime(\"21/01/2010\", '%d/%m/%Y')) & (log_cumulative_return.index < datetime.datetime.strptime(\"04/05/2012\", '%d/%m/%Y')), color=\"blue\", alpha=0.05, label=\"Validation\")\n",
    "plt.fill_between(log_cumulative_return.index, log_cumulative_return.min(), log_cumulative_return, where=log_cumulative_return.index >= datetime.datetime.strptime(\"04/05/2012\", '%d/%m/%Y'), color=\"green\", alpha=0.05, label=\"Test\")\n",
    "\n",
    "\n",
    "# log_cumulative_return.plot()\n",
    "# plt.title('Cumulative equally weighted index logreturns', fontsize = \"xx-large\")\n",
    "plt.xlabel('Date', fontsize = \"x-large\")\n",
    "plt.ylabel('Cumulative logreturns', fontsize = \"x-large\")\n",
    "\n",
    "# Increase the size of x and y tick labels\n",
    "plt.xticks(fontsize=\"large\")\n",
    "plt.yticks(fontsize=\"large\")\n",
    "\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "\n",
    "# # Format the x-axis to display years\n",
    "# years = mdates.YearLocator()   # every year\n",
    "# years_fmt = mdates.DateFormatter('%Y')\n",
    "# plt.gca().xaxis.set_major_locator(years)\n",
    "# plt.gca().xaxis.set_major_formatter(years_fmt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_split_number = 0.50 # With the larger SP500H index we might be able to get back to 2008 with a reasonable percentage (0.40 min)\n",
    "b_MinMaxScaler = True \n",
    "b_standardizer = False \n",
    "\n",
    "import Data_preparation\n",
    "importlib.reload(Data_preparation)\n",
    "\n",
    "# Scale features with MinMaxScaler or standardize features with StandardScaler\n",
    "df = Data_preparation.feature_scaling(df, b_MinMaxScaler, b_standardizer, percentage_split_number)\n",
    "\n",
    "# Creates splits and features and target dataframes\n",
    "X_in_sample, X_test, y_in_sample, y_test, X_train, X_val, y_train, y_val = Data_preparation.create_splits(df, percentage_split_number, model = \"not_lstm\", b_sentiment_score = b_sentiment_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dataframe which will be filled with all predictions\n",
    "df_classifications = df.loc[:, df.columns.isin([\"BarDate\", \"Ticker\", \"NextdayReturn\", 'Target'])]\n",
    "df_classifications = df_classifications.tail(len(X_test))\n",
    "\n",
    "# Creates dataframe which will be filled with all classifications\n",
    "df_predictions = df.loc[:, df.columns.isin([\"BarDate\", \"Ticker\", \"NextdayReturn\", 'Target'])]\n",
    "df_predictions = df_predictions.tail(len(X_test))\n",
    "\n",
    "# Creates dataframe which will be filled with all Accuracy values\n",
    "# df_accuracy = pd.DataFrame({}, index=[\"All\", \"Top 10\", \"Top 20\", \"Top decile\", \"Top quintile\", \"Second quintile\", \"Third quintile\", \"Fourth quintile\", \"Bottom quintile\", \"Bottom decile\", \"Bottom 20\", \"Bottom 10\"])\n",
    "df_accuracy = pd.DataFrame({}, index=[\"All\", \"Both 10\", \"Both 20\", \"Both 50\", \"Top 10\", \"Top 20\", \"Top 50\", \"Bottom 10\", \"Bottom 20\", \"Bottom 50\"])\n",
    "# Expanding window size for predictions in the test set +- 1 year\n",
    "window_size = 500*500\n",
    "\n",
    "# Change dtypes to 32 for memory efficiency\n",
    "df['PreviousdayReturn'] = df['PreviousdayReturn'].astype('float32')\n",
    "df['PreviousdayReturn_2'] = df['PreviousdayReturn_2'].astype('float32')\n",
    "df['PreviousdayReturn_3'] = df['PreviousdayReturn_3'].astype('float32')\n",
    "df[\"Target\"] = df[\"Target\"].astype('float32')\n",
    "df[\"SESI\"] = df[\"SESI\"].astype('float32')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Logistic_regression\n",
    "importlib.reload(Logistic_regression)\n",
    "\n",
    "ar_classifications_lr, ar_predictions_lr, model_lr = Logistic_regression.logistic_regression(window_size, y_in_sample, X_in_sample, y_test, X_test)\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_predictions[\"predictions_lr\"] = ar_predictions_lr\n",
    "# Add classifications to df_classifications\n",
    "df_classifications[\"classifications_lr\"] = ar_classifications_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"coefficients\", model_lr.coef_)\n",
    "print(\"intercept\", model_lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Evaluation_metrics\n",
    "importlib.reload(Evaluation_metrics)\n",
    "\n",
    "predictions_accuracy = Evaluation_metrics.prediction_metrics(df_classifications, str_model = \"lr\", str_single_metric = \"Accuracy\")\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_accuracy[\"LR\"] = predictions_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with ENet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ENet\n",
    "# importlib.reload(ENet)\n",
    "\n",
    "# # Specify the grid\n",
    "# grid = {\"l1_ratio\": [0.3, 0.5, 0.7], \"C\": np.logspace(start=-40, stop=-10, num=10, base=10)}\n",
    "# # Predictions \n",
    "# performances_ENet, best_model_ENet = ENet.ENet_tune(X_train, y_train, X_val, y_val, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # At the moment this returns 50% proba always, not sure if ENEt a good model for our features\n",
    "# # Or something is wrong with the code\n",
    "\n",
    "# import ENet\n",
    "# importlib.reload(ENet)\n",
    "\n",
    "# ar_classifications_enet, ar_predictions_enet, model_enet = ENet.ENet_test(best_model_ENet, window_size, y_in_sample, X_in_sample, y_test, X_test)\n",
    "\n",
    "# # Add predictions to df_predictions\n",
    "# df_predictions[\"predictions_enet\"] = ar_predictions_enet\n",
    "# # Add classifications to df_classifications\n",
    "# df_classifications[\"classifications_enet\"] = ar_classifications_enet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Evaluation_metrics\n",
    "# importlib.reload(Evaluation_metrics)\n",
    "\n",
    "# predictions_accuracy = Evaluation_metrics.prediction_metrics(df_classifications, str_model = \"enet\", str_single_metric = \"Accuracy\")\n",
    "\n",
    "# # Add predictions to df_predictions\n",
    "# df_accuracy[\"Elastic Net\"] = predictions_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "\n",
    "# Should be 1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 234min for PC1, (with tuning 4*4 combinations), with SESI\n",
    "# 120min for PC3, Without SESI\n",
    "# 160min for PC2, Without SESI\n",
    "\n",
    "import RF\n",
    "importlib.reload(RF)\n",
    "\n",
    "# Specify the grid \n",
    "# n_jobs = -1 makes use of all cores\n",
    "# When setting max_depth > 2 and more then 100000 rows, the kernel crashes\n",
    "# Try a different RF package?\n",
    "grid = {\"n_estimators\": [500, 700, 900],   \n",
    "        \"max_depth\": [2, 4, 6, 8]}\n",
    "                                                  #\"max_features\":    [3, 5, 10]} # Not needed because we only have 4 features\n",
    "# Predictions \n",
    "performances_RF, best_model_RF =  RF.RF_tune(X_train, y_train, X_val, y_val, grid)   #RF.RF_tune(X_train_1000, y_train_1000, X_val_1000, y_val_1000, grid)                       #RF.RF_tune(X_train, y_train, X_val, y_val, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 213min for PC1, with SESI\n",
    "# 120min for PC3, Without SESI\n",
    "# 220min for PC2, Without SESI\n",
    "\n",
    "import RF\n",
    "importlib.reload(RF)\n",
    "\n",
    "ar_classifications_rf, ar_predictions_rf, model_rf = RF.RF_test(best_model_RF, window_size, y_in_sample, X_in_sample, y_test, X_test)\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_predictions[\"predictions_rf\"] = ar_predictions_rf\n",
    "# Add classifications to df_classifications\n",
    "df_classifications[\"classifications_rf\"] = ar_classifications_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Evaluation_metrics\n",
    "importlib.reload(Evaluation_metrics)\n",
    "\n",
    "predictions_accuracy = Evaluation_metrics.prediction_metrics(df_classifications, str_model = \"rf\", str_single_metric = \"Accuracy\")\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_accuracy[\"RF\"] = predictions_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 210min for PC1, with SESI\n",
    "# 160min for PC3, without SESI\n",
    "# 180min for PC2, Without SESI\n",
    "\n",
    "import GBC\n",
    "importlib.reload(GBC)\n",
    "\n",
    "# Specify the grid \n",
    "grid = {\"n_estimators\": [100, 200, 500],\n",
    "        \"learning_rate\":[0.01, 0.1, 0.2],\n",
    "        \"max_depth\": [1, 2, 3]}\n",
    "# Predictions \n",
    "performances_GBC, best_model_GBC = GBC.GBC_tune(X_train, y_train, X_val, y_val, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... min for PC1, with SESI\n",
    "# 35 min for PC3, Without SESI\n",
    "# 40min for PC2, Without SESI\n",
    "\n",
    "import GBC\n",
    "importlib.reload(GBC)\n",
    "\n",
    "ar_classifications_gbc, ar_predictions_gbc, model_gbc = GBC.GBC_test(best_model_GBC, window_size, y_in_sample, X_in_sample, y_test, X_test)\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_predictions[\"predictions_gbc\"] = ar_predictions_gbc\n",
    "# Add classifications to df_classifications\n",
    "df_classifications[\"classifications_gbc\"] = ar_classifications_gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Evaluation_metrics\n",
    "importlib.reload(Evaluation_metrics)\n",
    "\n",
    "predictions_accuracy = Evaluation_metrics.prediction_metrics(df_classifications, str_model = \"gbc\", str_single_metric = \"Accuracy\")\n",
    "\n",
    "# Add predictions to df_predictions\n",
    "df_accuracy[\"GB\"] = predictions_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (and SimpleRNN) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load LSTM data from pickles\n",
    "directory = r\"C:\\Users\\BasPeeters\\OneDrive - FactorOrange.capital\\Master Thesis\\Dataframes and output\"\n",
    "folder_name = f\"lstm_sequences\\SESI={b_sentiment_score}_time_steps={time_steps}\"\n",
    "full_directory = os.path.join(directory, folder_name)\n",
    "\n",
    "names = ['X_in_sample_lstm', 'X_test_lstm', 'y_in_sample_lstm', 'y_test_lstm', 'X_train_lstm', 'X_val_lstm', 'y_train_lstm', 'y_val_lstm', \"df_y_test_lstm\"]\n",
    "variables = []\n",
    "for name in names:\n",
    "    with open(os.path.join(full_directory, f\"{name}.pkl\"), 'rb') as f:\n",
    "        var = pickle.load(f)\n",
    "        variables.append(var)\n",
    "\n",
    "X_in_sample_lstm, X_test_lstm, y_in_sample_lstm, y_test_lstm, X_train_lstm, X_val_lstm, y_train_lstm, y_val_lstm, df_y_test_lstm = variables\n",
    "del variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Takes 10min\n",
    "\n",
    "# # Unclear if this is needed, maybe using 3 lags with time steps is also good, test this\n",
    "# import LSTM\n",
    "# importlib.reload(LSTM)\n",
    "\n",
    "# # Creates splits and features and target dataframes\n",
    "# model = \"lstm\"\n",
    "# X_in_sample, X_test, y_in_sample, y_test, X_train, X_val, y_train, y_val = Data_preparation.create_splits(df, percentage_split_number, model, b_sentiment_score)\n",
    "\n",
    "# # Type should be multi_index_dataframe or 3D_array\n",
    "# X_in_sample_lstm, X_test_lstm, y_in_sample_lstm, y_test_lstm, X_train_lstm, X_val_lstm, y_train_lstm, y_val_lstm, df_y_test_lstm = LSTM.data_preparation(X_in_sample, X_test, y_in_sample, y_test, X_train, X_val, y_train, y_val, time_steps, b_sentiment_score, n_past_returns)\n",
    "\n",
    "# # Check if df_accuracy exists, if not create it\n",
    "# try:\n",
    "#     df_accuracy\n",
    "# except NameError:\n",
    "#     # Creates dataframe which will be filled with all Accuracy values\n",
    "#     df_accuracy = pd.DataFrame({}, index=[\"All\", \"Top 10\", \"Top 20\", \"Top decile\", \"Top quintile\", \"Second quintile\", \"Third quintile\", \"Fourth quintile\", \"Bottom quintile\", \"Bottom decile\", \"Bottom 20\", \"Bottom 10\"])\n",
    "\n",
    "\n",
    "# the X and y sets must be of equal length\n",
    "print(len(X_val_lstm))\n",
    "print(len(X_train_lstm))\n",
    "print(len(X_test_lstm))\n",
    "print(len(X_in_sample_lstm))\n",
    "\n",
    "print(len(y_val_lstm))\n",
    "print(len(y_train_lstm))\n",
    "print(len(y_test_lstm))\n",
    "print(len(y_in_sample_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The custom loss is not yet entirely perfect, now it considers batches of 500 stocks and calculates deciles from these 500, but not necessarily on 1 time period\n",
    "loss_function = 'binary_crossentropy'  # or \"custom_loss\"\n",
    "# Type of RNN: \"SimpleRNN\", \"LSTM\" or \"GRU\"\n",
    "str_nn_type = \"SimpleRNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn of printing for PC1??? It prints a lot of shit \n",
    "# 185min for PC1, with SESI\n",
    "\n",
    "# Takes #Epochs * #Tuning parameters * 1min to run  approximately\n",
    "\n",
    "import LSTM\n",
    "importlib.reload(LSTM)\n",
    "\n",
    "# Specify the grid \n",
    "# 18ms/step with dropout=0.1 and recurrent_dropout=0.1\n",
    "# 8ms/step with dropout=0.1 and recurrent_dropout=0.0\n",
    "grid ={ \"n_nodes\": [25, 64],\n",
    "        \"dropout\": [0.1], # Fischer uses 0.1                                                                             # Not much difference   \n",
    "        \"recurrent_dropout\": [0, 0.1],     #[0, 0.1], # Fischer uses 0.1 (after hyperparameter tuning)                                                                   # Adding recurrent_dropout makes the model 2.25 times slower   \n",
    "        \"learning_rate\": [0.001, 0.01],          #[0.1, 0.01, 0.001], # 0.001 is default for RMSprop and adam (also seemed best)                                              # learning_rate smaller should take longer to run but I do not see any difference here for values  0.1, 0.01, 0.001, 0.0001\n",
    "        \"batch_size\": [32, 64],     #[32, 64], # Default is 32                                                                              # Twice as fast with a twice as big batch size\n",
    "        \"optimizer\" : [\"RMSprop\"], # RMSprop Not default but used bij Fisher and good for RNN, default is adam             # adam seems eually fast as RMSprop                \n",
    "        \"sequence_length\": [time_steps] # 250 is roughly 1 year of data\n",
    "        } \n",
    "\n",
    "# Because the loss function needs this\n",
    "if loss_function == \"custom_loss\":\n",
    "    grid[\"batch_size\"] = [500]\n",
    "\n",
    "if str_nn_type == \"LSTM\":\n",
    "    # Trains model on validation set\n",
    "    performances_LSTM, best_model_LSTM = LSTM.LSTM_tune(X_train_lstm, y_train_lstm, X_val_lstm, y_val_lstm, grid, b_sentiment_score, n_past_returns, loss_function, str_nn_type) \n",
    "    # Plot the training and validation loss for the best model also return the best model\n",
    "    LSTM.plot_train_val_loss(performances_LSTM)    \n",
    "if str_nn_type == \"SimpleRNN\":\n",
    "    # Trains model on validation set\n",
    "    performances_RNN, best_model_RNN = LSTM.LSTM_tune(X_train_lstm, y_train_lstm, X_val_lstm, y_val_lstm, grid, b_sentiment_score, n_past_returns, loss_function, str_nn_type) \n",
    "    # Plot the training and validation loss for the best model also return the best model\n",
    "    LSTM.plot_train_val_loss(performances_RNN)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if str_nn_type == \"LSTM\":\n",
    "    # Creates dataframe which will be filled with all predictions\n",
    "    df_classifications_lstm = df_y_test_lstm.copy()\n",
    "    # Join the column NextdayReturn from df to df_classifications_lstm by BarDate and Ticker\n",
    "    df_classifications_lstm = df_classifications_lstm.join(df.loc[:, df.columns.isin([\"BarDate\", \"Ticker\", \"NextdayReturn\"])].set_index([\"BarDate\", \"Ticker\"]), on=[\"BarDate\", \"Ticker\"])\n",
    "    # Creates dataframe which will be filled with all classifications\n",
    "    df_predictions_lstm = df_y_test_lstm.copy()\n",
    "    # Join the column NextdayReturn from df to df_classifications_lstm by BarDate and Ticker\n",
    "    df_predictions_lstm = df_predictions_lstm.join(df.loc[:, df.columns.isin([\"BarDate\", \"Ticker\", \"NextdayReturn\"])].set_index([\"BarDate\", \"Ticker\"]), on=[\"BarDate\", \"Ticker\"])\n",
    "elif str_nn_type == \"SimpleRNN\":\n",
    "    # Creates dataframe which will be filled with all predictions\n",
    "    df_classifications_rnn = df_y_test_lstm.copy()\n",
    "    # Join the column NextdayReturn from df to df_classifications_lstm by BarDate and Ticker\n",
    "    df_classifications_rnn = df_classifications_rnn.join(df.loc[:, df.columns.isin([\"BarDate\", \"Ticker\", \"NextdayReturn\"])].set_index([\"BarDate\", \"Ticker\"]), on=[\"BarDate\", \"Ticker\"])\n",
    "    # Creates dataframe which will be filled with all classifications\n",
    "    df_predictions_rnn = df_y_test_lstm.copy()\n",
    "    # Join the column NextdayReturn from df to df_classifications_lstm by BarDate and Ticker\n",
    "    df_predictions_rnn = df_predictions_rnn.join(df.loc[:, df.columns.isin([\"BarDate\", \"Ticker\", \"NextdayReturn\"])].set_index([\"BarDate\", \"Ticker\"]), on=[\"BarDate\", \"Ticker\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 160 minutes to run\n",
    "# Takes #Epochs times 5 times 70sec to run\n",
    "\n",
    "import LSTM\n",
    "importlib.reload(LSTM)\n",
    "\n",
    "if str_nn_type == \"LSTM\":\n",
    "    ar_predictions_lstm, ar_classifications_lstm, model_lstm = LSTM.LSTM_test(best_model_LSTM, window_size, y_in_sample_lstm, X_in_sample_lstm, y_test_lstm, X_test_lstm, b_sentiment_score, n_past_returns, loss_function, str_nn_type) \n",
    "    df_predictions_lstm[\"predictions_lstm\"] = ar_predictions_lstm\n",
    "    df_classifications_lstm[\"classifications_lstm\"] = ar_classifications_lstm\n",
    "elif str_nn_type == \"SimpleRNN\":\n",
    "    ar_predictions_rnn, ar_classifications_rnn, model_rnn = LSTM.LSTM_test(best_model_RNN, window_size, y_in_sample_lstm, X_in_sample_lstm, y_test_lstm, X_test_lstm, b_sentiment_score, n_past_returns, loss_function, str_nn_type) \n",
    "    df_predictions_rnn[\"predictions_rnn\"] = ar_predictions_rnn\n",
    "    df_classifications_rnn[\"classifications_rnn\"] = ar_classifications_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joins LSTM predictions and classifications to df_predictions and df_classifications\n",
    "\n",
    "if str_nn_type == \"LSTM\":\n",
    "    # left join column classifications_lstm from df_classifications_lstm to df_classifications by BarDate and Ticker\n",
    "    df_classifications = df_classifications.join(df_classifications_lstm.loc[:, df_classifications_lstm.columns.isin([\"BarDate\", \"Ticker\", \"classifications_lstm\"])].set_index([\"BarDate\", \"Ticker\"]), on=[\"BarDate\", \"Ticker\"])\n",
    "    df_classifications = df_classifications.dropna()\n",
    "    del df_classifications_lstm\n",
    "    # left join column classifications_lstm from df_classifications_lstm to df_classifications by BarDate and Ticker\n",
    "    df_predictions = df_predictions.join(df_predictions_lstm.loc[:, df_predictions_lstm.columns.isin([\"BarDate\", \"Ticker\", \"predictions_lstm\"])].set_index([\"BarDate\", \"Ticker\"]), on=[\"BarDate\", \"Ticker\"])\n",
    "    df_predictions = df_predictions.dropna()\n",
    "    del df_predictions_lstm\n",
    "elif str_nn_type == \"SimpleRNN\":\n",
    "    # left join column classifications_lstm from df_classifications_lstm to df_classifications by BarDate and Ticker\n",
    "    df_classifications = df_classifications.join(df_classifications_rnn.loc[:, df_classifications_rnn.columns.isin([\"BarDate\", \"Ticker\", \"classifications_rnn\"])].set_index([\"BarDate\", \"Ticker\"]), on=[\"BarDate\", \"Ticker\"])\n",
    "    df_classifications = df_classifications.dropna()\n",
    "    del df_classifications_rnn\n",
    "    # left join column classifications_lstm from df_classifications_lstm to df_classifications by BarDate and Ticker\n",
    "    df_predictions = df_predictions.join(df_predictions_rnn.loc[:, df_predictions_rnn.columns.isin([\"BarDate\", \"Ticker\", \"predictions_rnn\"])].set_index([\"BarDate\", \"Ticker\"]), on=[\"BarDate\", \"Ticker\"])\n",
    "    df_predictions = df_predictions.dropna()\n",
    "    del df_predictions_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Evaluation_metrics\n",
    "importlib.reload(Evaluation_metrics)\n",
    "\n",
    "if str_nn_type == \"LSTM\":\n",
    "    predictions_accuracy = Evaluation_metrics.prediction_metrics(df_classifications, str_model = \"lstm\", str_single_metric = \"Accuracy\")\n",
    "    df_accuracy[\"LSTM\"] = predictions_accuracy       # If the backtest is so good but the accuracy in the top 20 not so then something is going wrong here!!!!!!!!! This can be tested by checking the number of longs and shorts above the median in portfolio \n",
    "\n",
    "elif str_nn_type == \"SimpleRNN\":\n",
    "    predictions_accuracy = Evaluation_metrics.prediction_metrics(df_classifications, str_model = \"rnn\", str_single_metric = \"Accuracy\")\n",
    "    df_accuracy[\"RNN\"] = predictions_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only the model from the last iteraton of the expanding window!\n",
    "# If we do Shap values or something then this must be mentioned!!!!!!!!!!!!!!!!!!!!!!!\n",
    "if str_nn_type == \"LSTM\":\n",
    "    model_lstm.summary()\n",
    "elif str_nn_type == \"SimpleRNN\":\n",
    "    model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall numpy\n",
    "# %pip install numpy==1.19.5\n",
    "\n",
    "# import shap\n",
    "\n",
    "# # We initialize the javascript for SHAP\n",
    "# shap.initjs()\n",
    "\n",
    "# X_train_lstm_sample = X_train_lstm[np.random.choice(X_train_lstm.shape[0], 1000, replace=False)]\n",
    "\n",
    "# # We create an explainer. DeepExplainer works well for deep learning models like LSTM.\n",
    "# explainer = shap.DeepExplainer(model_lstm, X_train_lstm_sample)\n",
    "\n",
    "# # Generate SHAP values\n",
    "# shap_values = explainer.shap_values(X_train_lstm_sample)\n",
    "\n",
    "# # Convert the input training set to a numpy array\n",
    "# X_train_array = np.array(X_train_lstm_sample)\n",
    "\n",
    "# # Summarize the effects of all the features\n",
    "# shap.summary_plot(shap_values, X_train_array)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification & Prediction analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_classifications[\"Target\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [\"classifications_lr\",  \"classifications_rf\",  \"classifications_gbc\", \"classifications_rnn\",  \"classifications_lstm\"]:\n",
    "    print(df_classifications[model].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts per quantile\n",
    "\n",
    "df_classifications_predictions = df_classifications.copy()\n",
    "# left join columns predictions_lr, predictions_rf, predictions_gbc, predictions_lstm from df_predictions to df_classifications_temp by BarDate and Ticker\n",
    "df_classifications_predictions = df_classifications_predictions.join(df_predictions.loc[:, df_predictions.columns.isin([\"BarDate\", \"Ticker\", \"predictions_lr\", \"predictions_rf\", \n",
    "                                                                                                                        \"predictions_gbc\", \"predictions_rnn\", \"predictions_lstm\"])].set_index([\"BarDate\", \"Ticker\"]), on=[\"BarDate\", \"Ticker\"])\n",
    "\n",
    "models = [\"lr\", \"rf\", \"gbc\", \"rnn\", \"lstm\"]\n",
    "dict_quantiles = {}\n",
    "for model in models:\n",
    "\n",
    "    # Group by date\n",
    "    grouped = df_classifications_predictions[[\"BarDate\", \"Ticker\", \"Target\", f\"classifications_{model}\", f\"predictions_{model}\"]].groupby(\"BarDate\")\n",
    "\n",
    "    # Select top 20, reset index, calculate str_single_metric of quantile\n",
    "    quantile_top = grouped.apply(lambda x: x.sort_values(by=f\"predictions_{model}\", \n",
    "                                                        ascending=False).iloc[: 20])\n",
    "    quantile_top = quantile_top.reset_index(drop=True)\n",
    "    # Select top 20, reset index, calculate str_single_metric of quantile\n",
    "    quantile_bottom = grouped.apply(lambda x: x.sort_values(by=f\"predictions_{model}\", \n",
    "                                                        ascending=False).iloc[int(len(x))-20 : ])\n",
    "    quantile_bottom = quantile_bottom.reset_index(drop=True)\n",
    "    quantile = pd.concat([quantile_top, quantile_bottom])\n",
    "    dict_quantiles[f\"quantile_{model}\"] = quantile\n",
    "\n",
    "    # Count the number of 0's and 1's for column classifications_lr in quantile\n",
    "    print(dict_quantiles[f\"quantile_{model}\"][f\"classifications_{model}\"].value_counts())\n",
    "\n",
    "    dict_quantiles[f\"quantile_{model}\"][f\"error_{model}\"] = np.where(dict_quantiles[f\"quantile_{model}\"][\"Target\"] == dict_quantiles[f\"quantile_{model}\"][f\"classifications_{model}\"], 0, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diebold Mariano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DM tests!!!!!!!!!\n",
    "import Evaluation_metrics\n",
    "importlib.reload(Evaluation_metrics)\n",
    "\n",
    "b_compare_SESI_to_no_SESI = False\n",
    "if b_compare_SESI_to_no_SESI == True:\n",
    "    df_classifications_temp = df_classifications_SESI.copy()\n",
    "    df_classifications_temp = df_classifications_temp.rename(columns={\"classifications_lstm\": \"classifications_lstm_SESI\", \"classifications_rnn\": \"classifications_rnn_SESI\", \"classifications_lr\": \"classifications_lr_SESI\", \n",
    "                                                                      \"classifications_rf\": \"classifications_rf_SESI\", \"classifications_gbc\": \"classifications_gbc_SESI\"})  \n",
    "    df_classifications_DM = df_classifications.merge(df_classifications_temp[[\"BarDate\", \"Ticker\", \"classifications_lr_SESI\", \"classifications_rf_SESI\", \"classifications_gbc_SESI\", \"classifications_rnn_SESI\", \"classifications_lstm_SESI\"]], how = \"left\", on = [\"BarDate\", \"Ticker\"])\n",
    "    df_predictions_temp = df_predictions_SESI.copy()\n",
    "    df_predictions_temp = df_predictions_temp.rename(columns={\"predictions_lstm\": \"predictions_lstm_SESI\", \"predictions_rnn\": \"predictions_rnn_SESI\", \"predictions_lr\": \"predictions_lr_SESI\", \"predictions_rf\": \"predictions_rf_SESI\", \"predictions_gbc\": \"predictions_gbc_SESI\"})  \n",
    "    df_predictions_DM = df_predictions.merge(df_predictions_temp[[\"BarDate\", \"Ticker\", \"predictions_lr_SESI\", \"predictions_rf_SESI\", \"predictions_gbc_SESI\", \"predictions_rnn_SESI\", \"predictions_lstm_SESI\"]], how = \"left\", on = [\"BarDate\", \"Ticker\"])\n",
    "else:\n",
    "    df_classifications_DM = df_classifications.copy()\n",
    "    df_predictions_DM = df_predictions.copy()\n",
    "\n",
    "str_dm_type = \"adjusted\"   # \"adjusted\" or \"classic\"\n",
    "str_sample = \"Top_bottom_20\"        # \"full\" or \"Top_bottom_20\" or \"Top_bottom_10\"\n",
    "# Do not use classic in combination with Top_bottom_20!!!!!!!!!!!!! Not possible\n",
    "dmTable, pValueTable = Evaluation_metrics.Diebold_Mariano(df_classifications_DM, df_predictions_DM, str_dm_type, str_sample, b_compare_SESI_to_no_SESI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio backtests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Portfolio part code needs to be adjusted maybe, seminar model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write code here to load gb, lr, rf predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_only = True\n",
    "# short_only = False\n",
    "\n",
    "# # Number of long and short stocks\n",
    "# n_both = 20\n",
    "# n_long = n_both\n",
    "# n_short = n_both\n",
    "\n",
    "# Choose which models we want\n",
    "backtest_models = [\"lr\", \"rf\", \"gbc\", \"rnn\", \"lstm\"]                          #\"NN1\", \"NN2\", \"NN3\", \"NN4\", \"NN5\"] \n",
    "legend_names = [\"LR\", \"RF\", \"GB\", \"RNN\", \"LSTM\"]\n",
    "\n",
    "# Choose weighting method (Equal weighted (Equal) or RankBased weighted (RankBased))\n",
    "# weighting_method = \"Equal\" \n",
    "\n",
    "dict_df_returns_portfolio = {}\n",
    "dict_df_metrics = {}\t\n",
    "weight_matrix_dict = {}\n",
    "           \n",
    "# # Cumulative return of stock index, used only in plot\n",
    "# stockindex_returns_cum = (1 + df_stockindex_returns[\"Return\"]).cumprod() - 1\n",
    "df_stockindex_returns = None\n",
    "\n",
    "# Define the colors for the lines\n",
    "# colors = ['red', 'blue', 'green', 'orange', 'red', 'blue', 'green', 'orange']\n",
    "colors = [ 'red', 'red', 'orange', 'orange', 'yellow', 'yellow',  'green', 'green', 'blue', 'blue']\n",
    "\n",
    "\n",
    "\n",
    "# Get factors and rf rate\n",
    "rf_rate_and_factors = pd.read_csv(os.path.join(r\"C:\\Users\\BasPeeters\\OneDrive - FactorOrange.capital\\Master Thesis\\Dataframes and output\", \"RF_rate_and_factors.csv\"))\n",
    "rf_rate_and_factors = rf_rate_and_factors['BarDate;Mkt-RF;SMB;HML;RMW;CMA;RF'].str.split(';', expand=True)\n",
    "rf_rate_and_factors.columns = ['BarDate', 'Mkt_min_RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']\n",
    "rf_rate_and_factors['BarDate'] = pd.to_datetime(rf_rate_and_factors['BarDate'], format='%d/%m/%Y')\n",
    "rf_rate_and_factors = rf_rate_and_factors[rf_rate_and_factors['BarDate'].isin(df_predictions['BarDate'])]\n",
    "rf_rate_and_factors.reset_index(drop=True, inplace=True)\n",
    "temp_LT_rev = pd.read_csv(os.path.join(r\"C:\\Users\\BasPeeters\\OneDrive - FactorOrange.capital\\Master Thesis\\Dataframes and output\", \"LT_Reversal_Factor.csv\"))\n",
    "temp_LT_rev = temp_LT_rev['BarDate;LT_Rev'].str.split(';', expand=True)\n",
    "temp_LT_rev.columns = ['BarDate', 'LT_Rev']\n",
    "temp_LT_rev['BarDate'] = pd.to_datetime(temp_LT_rev['BarDate'], format='%d/%m/%Y')\n",
    "temp_LT_rev = temp_LT_rev[temp_LT_rev['BarDate'].isin(df_predictions['BarDate'])]\n",
    "temp_LT_rev.reset_index(drop=True, inplace=True)\n",
    "temp_ST_rev = pd.read_csv(os.path.join(r\"C:\\Users\\BasPeeters\\OneDrive - FactorOrange.capital\\Master Thesis\\Dataframes and output\", \"ST_Reversal_Factor.csv\"))\n",
    "temp_ST_rev = temp_ST_rev['BarDate;ST_Rev'].str.split(';', expand=True)\n",
    "temp_ST_rev.columns = ['BarDate', 'ST_Rev']\n",
    "temp_ST_rev['BarDate'] = pd.to_datetime(temp_ST_rev['BarDate'], format='%d/%m/%Y')\n",
    "temp_ST_rev = temp_ST_rev[temp_ST_rev['BarDate'].isin(df_predictions['BarDate'])]\n",
    "temp_ST_rev.reset_index(drop=True, inplace=True)\n",
    "temp_MOM_rev = pd.read_csv(os.path.join(r\"C:\\Users\\BasPeeters\\OneDrive - FactorOrange.capital\\Master Thesis\\Dataframes and output\", \"Momentum_Factor.csv\"))\n",
    "temp_MOM_rev = temp_MOM_rev['BarDate;Mom   '].str.split(';', expand=True)\n",
    "temp_MOM_rev.columns = ['BarDate', 'Mom']\n",
    "temp_MOM_rev['BarDate'] = pd.to_datetime(temp_MOM_rev['BarDate'], format='%d/%m/%Y')\n",
    "temp_MOM_rev = temp_MOM_rev[temp_MOM_rev['BarDate'].isin(df_predictions['BarDate'])]\n",
    "temp_MOM_rev.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "rf_rate_and_factors[\"ST_Rev\"] = temp_ST_rev[\"ST_Rev\"]\n",
    "rf_rate_and_factors[\"LT_Rev\"] = temp_LT_rev[\"LT_Rev\"]\n",
    "rf_rate_and_factors[\"Mom\"] = temp_MOM_rev[\"Mom\"]\n",
    "rf_rate_and_factors[[\"Mkt_min_RF\", \"SMB\", \"HML\", \"RF\", \"ST_Rev\", \"LT_Rev\", \"Mom\"]] = rf_rate_and_factors[[\"Mkt_min_RF\", \"SMB\", \"HML\", \"RF\", \"ST_Rev\", \"LT_Rev\", \"Mom\"]].astype('float32')\n",
    "rf_rate_and_factors[['Mkt_min_RF', 'SMB', 'HML','RF', \"ST_Rev\", \"LT_Rev\", \"Mom\"]] = rf_rate_and_factors[['Mkt_min_RF', 'SMB', 'HML', 'RF', \"ST_Rev\", \"LT_Rev\", \"Mom\"]] / 100\n",
    "\n",
    "# Only one spot has NaN values for three factors\n",
    "rf_rate_and_factors = rf_rate_and_factors.fillna(0)\n",
    "rf_rate_and_factors.drop(columns=[\"RMW\", \"CMA\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 5:30 to run\n",
    "\n",
    "# # Creates a matrix with all actual returns only \n",
    "# # The last columns are not the last dates. These all seem NaN values.............Something must change...........................\n",
    "# stocks = df_predictions['Ticker'].unique()\n",
    "# returnsMatrix = pd.DataFrame(index=df_predictions['BarDate'].unique(), columns=stocks)\n",
    "# for stock in stocks:\n",
    "#     stock_df = df_predictions[df_predictions['Ticker'] == stock]\n",
    "#     for index, row in stock_df.iterrows():\n",
    "#         returnsMatrix.at[row['BarDate'], stock] = row['NextdayReturn']\n",
    "\n",
    "# Load returnsMatrix\n",
    "returnsMatrix = pd.read_csv(os.path.join(r\"C:\\Users\\BasPeeters\\OneDrive - FactorOrange.capital\\Master Thesis\\Dataframes and output\", \"returnsMatrix.csv\"))\n",
    "returnsMatrix = returnsMatrix.set_index('Unnamed: 0')\n",
    "returnsMatrix.index = pd.to_datetime(returnsMatrix.index, format='%Y-%m-%d')\n",
    "returnsMatrix = returnsMatrix[returnsMatrix.index.isin(df_predictions['BarDate'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_classifications = df_classifications[df_classifications[\"BarDate\"] < datetime.datetime.strptime(\"28/05/2012\", '%d/%m/%Y')]\n",
    "# df_predictions = df_predictions[df_predictions[\"BarDate\"] < datetime.datetime.strptime(\"28/05/2012\", '%d/%m/%Y')]\n",
    "# returnsMatrix = returnsMatrix[returnsMatrix.index < datetime.datetime.strptime(\"28/05/2012\", '%d/%m/%Y')]\n",
    "# rf_rate_and_factors = rf_rate_and_factors[rf_rate_and_factors[\"BarDate\"] < datetime.datetime.strptime(\"28/05/2012\", '%d/%m/%Y')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = df.copy()\n",
    "# df_test = df_test[  df_test[\"BarDate\"] < datetime.datetime.strptime(\"30/05/2012\", '%d/%m/%Y')]\n",
    "# df_test = df_test[  df_test[\"BarDate\"] >= datetime.datetime.strptime(\"10/05/2012\", '%d/%m/%Y')]\n",
    "# df_test[df_test[\"Ticker\"] == \"AAPL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of unique elements of BarDate\n",
    "df_predictions['BarDate'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_tradingcosts = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "# Extra metrics take long to run\n",
    "# +- 72min for full run with 10,20,50 and all models\n",
    "\n",
    "import Portfolio_backtester\n",
    "importlib.reload(Portfolio_backtester)\n",
    "\n",
    "# Number of long and short stocks\n",
    "for n_both in [10,20,50]:        #    tqdm([10, 20, 50]):                   #([10, 20, 50]):\n",
    "\n",
    "    #n_both = 20\n",
    "    n_long = n_both\n",
    "    n_short = n_both\n",
    "    n_constituents = n_both * 2\n",
    "    # Create a loop which first sets long_only = True and then short_only = True and then both = False\n",
    "    for long_only in [False]:                              #tqdm([True, False]):\n",
    "        for short_only in [False]:                               #[True, False]:\n",
    "            if (long_only == True) & (short_only == True):\n",
    "                continue\n",
    "            if long_only == True:\n",
    "                str_side = \"long\"\n",
    "            if short_only == True:\n",
    "                str_side = \"short\"\n",
    "            if (long_only == False) & (short_only == False):\n",
    "                str_side = \"long_short\"\n",
    "\n",
    "            # Plot the cumulative portfolio return\n",
    "            plt.subplots(figsize=(20,10))\n",
    "\n",
    "            # Use this for color of the lines\n",
    "            i = 0\n",
    "            # for weighting_method in tqdm([\"Equal\", \"RankBased\"]):\n",
    "            for weighting_method in [\"Equal\"]:\n",
    "                dict_df_metrics[f\"{weighting_method}_{n_both}_{str_side}\"] = pd.DataFrame({}, index=[\"mean_return\", 'std_dev', \"t_stat\", \"p_value\", \"sharpe_ratio\", 'max_drawdown', 'max_1_day_loss', \"turnover\"])\n",
    "                dict_df_returns_portfolio[f\"{weighting_method}_{n_both}_{str_side}\"] = pd.DataFrame({\"BarDate\": df_predictions[\"BarDate\"].unique()})\n",
    "                for backtest_model in tqdm(backtest_models):\n",
    "                    # Predictions are now stored in two different objects\n",
    "                    # if backtest_model == \"lstm\":\n",
    "                    #     df_predictions_backtest = df_predictions_lstm\n",
    "                    #     df_returns_portfolio_backtest = df_returns_portfolio_lstm\n",
    "\n",
    "                    predictions_test_set, df_returns_portfolio, str_returns_portfolio_cum, str_logreturns_portfolio_cum, weightMatrix = Portfolio_backtester.backtest(df_predictions,n_long,n_short,backtest_model,\n",
    "                                                                                    weighting_method, dict_df_returns_portfolio[f\"{weighting_method}_{n_both}_{str_side}\"], returnsMatrix, long_only, short_only, b_tradingcosts)\n",
    "                    weight_matrix_dict[f\"{weighting_method}_{n_both}_{str_side}\"] = weightMatrix     # Weight matrix does not seem correct!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            \n",
    "                    if weighting_method == \"Equal\":\n",
    "                        plt.plot(pd.to_datetime(df_returns_portfolio[\"BarDate\"]), df_returns_portfolio[str_logreturns_portfolio_cum], color = colors[i], label=str(f\"{backtest_model} {weighting_method}\"))\n",
    "                    elif(weighting_method == \"RankBased\"):\n",
    "                        plt.plot(pd.to_datetime(df_returns_portfolio[\"BarDate\"]), df_returns_portfolio[str_logreturns_portfolio_cum], color = colors[i], linestyle = '--', label=str(f\"{backtest_model} {weighting_method}\"))\n",
    "                    # Adds portfolio returns to the dictionary with all portfolio returns\n",
    "                    dict_df_returns_portfolio[f\"{weighting_method}_{n_both}_{str_side}\"] = df_returns_portfolio\n",
    "                    # Gives metrics, now only for the full long-short portfolio\n",
    "                    dict_df_metrics[f\"{weighting_method}_{n_both}_{str_side}\"] = Portfolio_backtester.metrics(dict_df_returns_portfolio[f\"{weighting_method}_{n_both}_{str_side}\"], backtest_model, dict_df_metrics[f\"{weighting_method}_{n_both}_{str_side}\"], df_stockindex_returns, returnsMatrix, weight_matrix_dict[f\"{weighting_method}_{n_both}_{str_side}\"], rf_rate_and_factors)\n",
    "                    i = i + 1\n",
    "\n",
    "            #plt.plot(pd.to_datetime(df_stockindex_returns[\"Date\"]), stockindex_returns_cum) # Plots long only index\n",
    "            plt.axvspan(datetime.datetime.strptime(\"29/02/2020\", '%d/%m/%Y').date(), datetime.datetime.strptime(\"31/03/2020\", '%d/%m/%Y').date(), color=\"grey\", alpha=0.5)\n",
    "            # plt.axvspan(datetime.datetime.strptime(\"31/12/2007\", '%d/%m/%Y').date(), datetime.datetime.strptime(\"31/05/2009\", '%d/%m/%Y').date(), color=\"grey\", alpha=0.5)\n",
    "\n",
    "            plt.xlabel(xlabel=\"Date\", fontsize = \"x-large\")\n",
    "            if long_only == True:\n",
    "                plt.ylabel(ylabel=\"Cumulative (long) logreturns\", fontsize = \"x-large\")\n",
    "            if short_only == True:\n",
    "                plt.ylabel(ylabel=\"Cumulative (short) logreturns\", fontsize = \"x-large\")\n",
    "            if (long_only == False) & (short_only == False):\n",
    "                plt.ylabel(ylabel=\"Cumulative logreturns\", fontsize = \"x-large\")\n",
    "            plt.title(label=f\"Cumulative {n_constituents}-stock portfolio logreturns\", fontsize = \"xx-large\")  \n",
    "            plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "            #plt.legend(['lr', 'rf', 'gb', 'lstm', 'lr_rank', 'rf_rank', 'gb_rank', 'lstm_rank'] , loc = 'upper center', ncols = 4, fontsize = 'large')    # Without specifying loc it chooses the best location\n",
    "            plt.legend(legend_names  , loc = 'upper center', ncols = len(legend_names), fontsize = 'large')    # Without specifying loc it chooses the best location\n",
    "\n",
    "            # Format the x-axis to display years\n",
    "            years = mdates.YearLocator()   # every year\n",
    "            years_fmt = mdates.DateFormatter('%Y')\n",
    "            plt.gca().xaxis.set_major_locator(years)\n",
    "            plt.gca().xaxis.set_major_formatter(years_fmt)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only calculates metrics\n",
    "\n",
    "import Portfolio_backtester\n",
    "importlib.reload(Portfolio_backtester) \n",
    "\n",
    "dict_df_metrics = {}\n",
    "for n_both in [10]:                                              #[10, 20, 50]:\n",
    "    for long_only in [False]:                 #[True, False]:\n",
    "        for short_only in [False]:             # [True, False]:\n",
    "            if (long_only == True) & (short_only == True):\n",
    "                continue\n",
    "            if long_only == True:\n",
    "                str_side = \"long\"\n",
    "            if short_only == True:\n",
    "                str_side = \"short\"\n",
    "            if (long_only == False) & (short_only == False):\n",
    "                str_side = \"long_short\"                \n",
    "            for weighting_method in [\"Equal\"]:\n",
    "                dict_df_metrics[f\"{weighting_method}_{n_both}_{str_side}\"] = pd.DataFrame({}, index=[\"mean_return\", 'std_dev', 't_stat', 'p_value', \"sharpe_ratio\", 'max_drawdown', 'max_1_day_loss', \"turnover\"])    \n",
    "                for backtest_model in (backtest_models):\n",
    "                    dict_df_metrics[f\"{weighting_method}_{n_both}_{str_side}\"] = Portfolio_backtester.metrics(dict_df_returns_portfolio[f\"{weighting_method}_{n_both}_{str_side}\"], backtest_model, dict_df_metrics[f\"{weighting_method}_{n_both}_{str_side}\"], df_stockindex_returns, returnsMatrix, weight_matrix_dict[f\"{weighting_method}_{n_both}_{str_side}\"], rf_rate_and_factors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Equal_10_long_short':                       lr        rf       gbc       rnn      lstm\n",
       " mean_return     0.000100  0.000278  0.000254  0.000200  0.000146\n",
       " std_dev         0.009093  0.008334  0.007512  0.007929  0.006528\n",
       " t_stat          0.571676  1.741659  1.767219  1.318367  1.166831\n",
       " p_value         0.567589  0.081681  0.077303  0.187492  0.243381\n",
       " sharpe_ratio    0.125468  0.476794  0.478789  0.345401  0.287428\n",
       " max_drawdown   -0.301945 -0.218146 -0.166789 -0.173616 -0.165140\n",
       " max_1_day_loss -0.051788 -0.050143 -0.048791 -0.045803 -0.037054\n",
       " turnover        1.520923  1.520923  1.520923  1.520923  1.520923}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only plots figures\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "import Portfolio_backtester\n",
    "importlib.reload(Portfolio_backtester)\n",
    "\n",
    "backtest_models = [\"lr\", \"rf\", \"gbc\", \"rnn\", \"lstm\"]                         \n",
    "# legend_names = [\"LR_SESI\", \"RF_SESI\", \"GB_SESI\", \"RNN_SESI\", \"LSTM_SESI\", \"LR\", \"RF\", \"GB\", \"RNN\", \"LSTM\"]\n",
    "legend_names = [\"LR_SESI\", \"LR\", \"RF_SESI\", \"RF\", \"GB_SESI\", \"GB\", \"RNN_SESI\", \"RNN\", \"LSTM_SESI\", \"LSTM\"] \n",
    "# colors = [(1.0, 0.0, 0.0),(1.0, 0.0, 0.0), (0.75, 0.0, 0.25),(0.75, 0.0, 0.25), (0.5, 0.0, 0.5),(0.5, 0.0, 0.5), (0.25, 0.0, 0.75),(0.25, 0.0, 0.75), (0.0, 0.0, 1.0), (0.0, 0.0, 1.0)]     # from red to blue \n",
    "# colors = [(0.0, 0.0, 0), (0.0, 0.0, 0), (0.0, 0.0, 0.40), (0.0, 0.0, 0.40), (0.0, 0.0, 0.80), (0.0, 0.0, 0.80), (0.4, 0.4, 1), (0.4, 0.4, 1), (0.8, 0.8, 1.0), (0.8, 0.8, 1.0)]        # Black to light blue        # Best colours\n",
    "colors = [ 'red', 'red', 'orange', 'orange', 'yellow', 'yellow',  'green', 'green', 'blue', 'blue']\n",
    "\n",
    "str_side = \"long_short\"\n",
    "n_both = 10\n",
    "n_constituents = 2*n_both\n",
    "\n",
    "\n",
    "\n",
    "plt.subplots(figsize=(15,6))\n",
    "i = 0\n",
    "for backtest_model in tqdm(backtest_models):\n",
    "    for weighting_method in [\"Equal\"]:\n",
    "        for b_SESI in [True, False]:\n",
    "            if b_SESI == True:\n",
    "                df_returns_portfolio = df_returns_portfolio_Equal_10_long_short_SESI.copy()\n",
    "            else:\n",
    "                df_returns_portfolio = df_returns_portfolio_Equal_10_long_short.copy()\n",
    "\n",
    "            str_logreturns_portfolio_cum = \"logreturns_\" + backtest_model + \"_portfolio_cum\"\n",
    "\n",
    "            if b_SESI == True:\n",
    "                plt.plot(pd.to_datetime(df_returns_portfolio[\"BarDate\"]), df_returns_portfolio[str_logreturns_portfolio_cum], color = colors[i], label=str(f\"{backtest_model} {weighting_method}\"))\n",
    "            elif b_SESI == False:\n",
    "                plt.plot(pd.to_datetime(df_returns_portfolio[\"BarDate\"]), df_returns_portfolio[str_logreturns_portfolio_cum], color = colors[i], linestyle = '--', label=str(f\"{backtest_model} {weighting_method}\"))\n",
    "            # Adds portfolio returns to the dictionary with all portfolio returns\n",
    "            i = i + 1\n",
    "#plt.plot(pd.to_datetime(df_stockindex_returns[\"Date\"]), stockindex_returns_cum) # Plots long only index\n",
    "plt.axvspan(datetime.datetime.strptime(\"29/02/2020\", '%d/%m/%Y').date(), datetime.datetime.strptime(\"31/03/2020\", '%d/%m/%Y').date(), color=\"grey\", alpha=0.5)\n",
    "# plt.axvspan(datetime.datetime.strptime(\"31/12/2007\", '%d/%m/%Y').date(), datetime.datetime.strptime(\"31/05/2009\", '%d/%m/%Y').date(), color=\"grey\", alpha=0.5)\n",
    "\n",
    "plt.xlabel(xlabel=\"Date\", fontsize = \"x-large\")\n",
    "if (str_side == \"long\"):\n",
    "    plt.ylabel(ylabel=\"Cumulative (long) logreturns\", fontsize = \"x-large\")\n",
    "if (str_side == \"short\"):\n",
    "    plt.ylabel(ylabel=\"Cumulative (short) logreturns\", fontsize = \"x-large\")\n",
    "if (str_side == \"long_short\"):\n",
    "    plt.ylabel(ylabel=\"Cumulative logreturns\", fontsize = \"x-large\")\n",
    "# plt.title(label=f\"Cumulative {n_constituents}-stock portfolio logreturns\", fontsize = \"xx-large\")  \n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "#plt.legend(['lr', 'rf', 'gb', 'lstm', 'lr_rank', 'rf_rank', 'gb_rank', 'lstm_rank'] , loc = 'upper center', ncols = 4, fontsize = 'large')    # Without specifying loc it chooses the best location\n",
    "plt.legend(legend_names  , loc = 'upper center', ncols = 5, fontsize = 'medium')    # Without specifying loc it chooses the best location\n",
    "\n",
    "# Increase the size of x and y tick labels\n",
    "plt.xticks(fontsize=\"large\")\n",
    "plt.yticks(fontsize=\"large\")\n",
    "\n",
    "# Format the x-axis to display years\n",
    "years = mdates.YearLocator()   # every year\n",
    "years_fmt = mdates.DateFormatter('%Y')\n",
    "plt.gca().xaxis.set_major_locator(years)\n",
    "plt.gca().xaxis.set_major_formatter(years_fmt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots both subsamples next to each other!!!!!!!!!!!\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "for idx, subsample in enumerate([\"first_half\", \"second_half\"]):\n",
    "    plt.subplot(1, 2, idx + 1)  # create subplot\n",
    "    i = 0\n",
    "    for backtest_model in tqdm(backtest_models):\n",
    "        for weighting_method in [\"Equal\"]:\n",
    "            for b_SESI in [True, False]:\n",
    "                if b_SESI == True:\n",
    "                    df_returns_portfolio = df_returns_portfolio_Equal_10_long_short_SESI.copy()\n",
    "                else:\n",
    "                    df_returns_portfolio = df_returns_portfolio_Equal_10_long_short.copy()\n",
    "\n",
    "                if subsample == \"second_half\":\n",
    "                    df_returns_portfolio = df_returns_portfolio[df_returns_portfolio[\"BarDate\"] > datetime.datetime.strptime(\"31/12/2019\", '%d/%m/%Y')]\n",
    "                elif subsample == \"first_half\":\n",
    "                    df_returns_portfolio = df_returns_portfolio[df_returns_portfolio[\"BarDate\"] <= datetime.datetime.strptime(\"01/01/2020\", '%d/%m/%Y')]      \n",
    "                \n",
    "                str_returns_portfolio = \"returns_\" + backtest_model + \"_portfolio\"\n",
    "                str_logreturns_portfolio_cum = \"logreturns_\" + backtest_model + \"_portfolio_cum\"\n",
    "                df_returns_portfolio[str_logreturns_portfolio_cum] = np.log(1 + df_returns_portfolio[str_returns_portfolio]).cumsum()\n",
    "\n",
    "                if b_SESI == True:\n",
    "                    plt.plot(pd.to_datetime(df_returns_portfolio[\"BarDate\"]), df_returns_portfolio[str_logreturns_portfolio_cum], color = colors[i], label=str(f\"{backtest_model} {weighting_method}\"))\n",
    "                elif b_SESI == False:\n",
    "                    plt.plot(pd.to_datetime(df_returns_portfolio[\"BarDate\"]), df_returns_portfolio[str_logreturns_portfolio_cum], color = colors[i], linestyle = '--', label=str(f\"{backtest_model} {weighting_method}\"))\n",
    "                i = i + 1\n",
    "                \n",
    "    plt.axvspan(datetime.datetime.strptime(\"29/02/2020\", '%d/%m/%Y').date(), datetime.datetime.strptime(\"31/03/2020\", '%d/%m/%Y').date(), color=\"grey\", alpha=0.5)\n",
    "    plt.xlabel(xlabel=\"Date\", fontsize = \"x-large\")\n",
    "    \n",
    "    if subsample == \"first_half\":\n",
    "        if (str_side == \"long\"):\n",
    "            plt.ylabel(ylabel=\"Cumulative (long) logreturns\", fontsize = \"x-large\")\n",
    "        if (str_side == \"short\"):\n",
    "            plt.ylabel(ylabel=\"Cumulative (short) logreturns\", fontsize = \"x-large\")\n",
    "        if (str_side == \"long_short\"):\n",
    "            plt.ylabel(ylabel=\"Cumulative logreturns\", fontsize = \"x-large\")\n",
    "        # plt.legend(legend_names  , loc = 'upper center', ncols = 5, fontsize = 'medium')\n",
    "        plt.legend(legend_names, loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=5, fancybox=True, shadow=True)\n",
    "    \n",
    "    plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "   \n",
    "\n",
    "    plt.xticks(fontsize=\"large\")\n",
    "    plt.yticks(fontsize=\"large\")\n",
    "\n",
    "    years = mdates.YearLocator()\n",
    "    years_fmt = mdates.DateFormatter('%Y')\n",
    "    plt.gca().xaxis.set_major_locator(years)\n",
    "    plt.gca().xaxis.set_major_formatter(years_fmt)\n",
    "\n",
    "plt.tight_layout()  # ensure subplots do not overlap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_df_metrics[\"Equal_20_long_short\"])\n",
    "# Check the distribution of column \"returns_lr_portfolio\" in dict_df_returns_portfolio[\"Equal_20_long_short\"]\n",
    "print(dict_df_returns_portfolio[\"Equal_20_long_short\"][\"returns_lstm_portfolio\"].hist(bins=100))\n",
    "# Check the distribution of column \"returns_lr_portfolio\" in dict_df_returns_portfolio[\"Equal_20_long_short\"]\n",
    "print(dict_df_returns_portfolio[\"Equal_20_long_short\"][\"returns_lstm_portfolio\"].describe())\n",
    "# Calculate the skewness and kurtosis of dict_df_returns_portfolio[\"Equal_20_long_short\"][\"returns_lstm_portfolio\"]\n",
    "# Skewness can be negative because of positive mean\n",
    "print(skew(dict_df_returns_portfolio[\"Equal_20_long_short\"][[\"returns_lr_portfolio\" ,\"returns_rf_portfolio\", \"returns_gbc_portfolio\", \"returns_rnn_portfolio\" ,\"returns_lstm_portfolio\"]], axis=0, bias=True))\n",
    "# Count the number of positive values in a series\n",
    "print(len(dict_df_returns_portfolio[\"Equal_20_long_short\"][\"returns_lstm_portfolio\"]))\n",
    "print(dict_df_returns_portfolio[\"Equal_20_long_short\"][\"returns_lstm_portfolio\"][dict_df_returns_portfolio[\"Equal_20_long_short\"][\"returns_lstm_portfolio\"] > 0].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of rows for each year of the dataframe\n",
    "print(\"Trading days per year\")\n",
    "dict_df_returns_portfolio[\"Equal_20_long_short\"].groupby(dict_df_returns_portfolio[\"Equal_20_long_short\"][\"BarDate\"].dt.year).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change for new file!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "b_sentiment_score = False\n",
    "LSTM_model_type = \"LSTM_model_1\"\n",
    "time_steps = \"end\"\n",
    "# n_both\n",
    "# weighting_method\n",
    "n_nodes = \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Creating the directory name\n",
    "directory = r\"C:\\Users\\BasPeeters\\OneDrive - FactorOrange.capital\\Master Thesis\\Dataframes and output_new\"\n",
    "folder_name = f\"SESI={b_sentiment_score}_{LSTM_model_type}_time_steps={time_steps}_n_nodes={n_nodes}\"\n",
    "full_directory = os.path.join(directory, folder_name)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(full_directory):\n",
    "    os.makedirs(full_directory)\n",
    "\n",
    "# full_directory_final_results = os.path.join(directory, \"Final results\")\n",
    "# # # Create the directory if it doesn't exist\n",
    "# if not os.path.exists(full_directory_final_results):\n",
    "#     os.makedirs(full_directory_final_results)\n",
    "\n",
    "\n",
    "# Save weight matrix dictionary \n",
    "for key in weight_matrix_dict:\n",
    "    weight_matrix_dict[key].to_csv(os.path.join(directory, f\"weight_matrix_dict_{key}.csv\"))\n",
    "\n",
    "# Save regression on risk factor results\n",
    "\n",
    "# for key in dict_regression_results:\n",
    "#     dict_regression_results[key].to_csv(os.path.join(full_directory_final_results, f\"regression_results_{key}.csv\"))\n",
    "\n",
    "\n",
    "\n",
    "# Save DM tables\n",
    "# try:\n",
    "#     dmTable.to_csv(os.path.join(directory, \"DM table.csv\"))\n",
    "# except:\n",
    "#     print(\"dmTable does not exist\")\n",
    "# try:\n",
    "#     pValueTable.to_csv(os.path.join(directory, \"pValueTable_temp.csv\"))\n",
    "# except:\n",
    "#     print(\"pValueTable does not exist\")\n",
    "\n",
    "# # # # Save predicitons and classifications\n",
    "# try:\n",
    "#     df_predictions.to_pickle(os.path.join(full_directory, \"df_predictions.pkl\"))\n",
    "# except:\n",
    "#     print(\"df_predictions does not exist\")\n",
    "# try:\n",
    "#     df_classifications.to_pickle(os.path.join(full_directory, \"df_classifications.pkl\"))\n",
    "# except:\n",
    "#     print(\"df_classifications does not exist\")\n",
    "\n",
    "# # Save accuracy \n",
    "# df_accuracy.to_csv(os.path.join(full_directory, \"df_accuracy.csv\"))\n",
    "\n",
    "# # # Save model parameters\n",
    "# try: \n",
    "#     dump(best_model_LSTM, os.path.join(full_directory, 'best_model_LSTM.joblib'))\n",
    "# except:\n",
    "#     print(\"best_model_LSTM does not exist\")\t\n",
    "# try:\n",
    "#     dump(best_model_GBC, os.path.join(full_directory, 'best_model_GBC.joblib'))\n",
    "# except:\n",
    "#     print(\"best_model_GBC does not exist\")\n",
    "# try:\n",
    "#     dump(best_model_RF, os.path.join(full_directory, 'best_model_RF.joblib'))\n",
    "# except:\n",
    "#     print(\"best_model_RF does not exist\")\n",
    "# try:\n",
    "#     dump(best_model_RNN, os.path.join(full_directory, 'best_model_RNN.joblib'))\n",
    "# except:\n",
    "#     print(\"best_model_RNN does not exist\")\n",
    "\n",
    "\n",
    "# # Save Actual lstm model\n",
    "# try:\n",
    "#     model_name = f\"SESI={b_sentiment_score}_{LSTM_model_type}_time_steps={time_steps}_n_nodes={n_nodes}\"\n",
    "#     model_lstm.save(os.path.join(full_directory,f\"{model_name}.h5\"))\n",
    "# except:\n",
    "#     print(\"model_lstm does not exist\")\n",
    "\n",
    "# Portfolio metrics\n",
    "\n",
    "full_directory_portfolio = os.path.join(full_directory, \"Portfolio\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(full_directory_portfolio):\n",
    "    os.makedirs(full_directory_portfolio)\n",
    "\n",
    "try:\n",
    "    for key in dict_df_metrics:\n",
    "        dict_df_metrics[key].to_csv(os.path.join(full_directory_portfolio, f'df_metrics_{key}.csv'))\n",
    "except:\n",
    "    print(\"dict_df_metrics does not exist\")\n",
    "\n",
    "try:\n",
    "    for key in dict_df_returns_portfolio:\n",
    "        dict_df_returns_portfolio[key].to_pickle(os.path.join(full_directory_portfolio, f\"df_returns_portfolio_{key}.pkl\"))\n",
    "except:\n",
    "    print(\"dict_df_returns_portfolio does not exist\")\n",
    "\n",
    "print(f\"All data is saved to {full_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_metrics[\"Equal_10_long_short\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM sequences\n",
    "\n",
    "# Creating the directory name\n",
    "directory = r\"C:\\Users\\BasPeeters\\OneDrive - FactorOrange.capital\\Master Thesis\\Dataframes and output\\lstm_sequences\"\n",
    "folder_name = f\"SESI={b_sentiment_score}_time_steps={time_steps}\"\n",
    "full_directory = os.path.join(directory, folder_name)\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(full_directory):\n",
    "    os.makedirs(full_directory)\n",
    "\n",
    "variables = [X_in_sample_lstm, X_test_lstm, y_in_sample_lstm, y_test_lstm, X_train_lstm, X_val_lstm, y_train_lstm, y_val_lstm, df_y_test_lstm]\n",
    "names = ['X_in_sample_lstm', 'X_test_lstm', 'y_in_sample_lstm', 'y_test_lstm', 'X_train_lstm', 'X_val_lstm', 'y_train_lstm', 'y_val_lstm', \"df_y_test_lstm\"]\n",
    "\n",
    "for var, name in zip(variables, names):\n",
    "    with open(os.path.join(full_directory, f\"{name}.pkl\"), 'wb') as f:\n",
    "        pickle.dump(var, f)\n",
    "del variables, names, var, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import importlib\n",
    "from warnings import simplefilter\n",
    "from six import StringIO\n",
    "from IPython.display import Image\n",
    "\n",
    "# Reload external files\n",
    "import importlib\n",
    "\n",
    "# Mute warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change for new file!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "b_sentiment_score = False\n",
    "LSTM_model_type = \"LSTM_model_1\"\n",
    "time_steps = \"end\"\n",
    "# n_both\n",
    "# weighting_method\n",
    "n_nodes = \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Join the columns classifications_lr, classifications_rf and classifications_gb from df_classifications to df_classifications_10 by BarDate and Ticker\n",
    "# df_predictions_test = df_predictions_10.merge(df_predictions[[\"BarDate\", \"Ticker\", \"predictions_lr\", \"predictions_rf\", \"predictions_gbc\"]], how = \"left\", on = [\"BarDate\", \"Ticker\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "# Creating the directory name\n",
    "directory = r\"C:\\Users\\BasPeeters\\OneDrive - FactorOrange.capital\\Master Thesis\\Dataframes and output\"\n",
    "folder_name = f\"SESI={b_sentiment_score}_{LSTM_model_type}_time_steps={time_steps}_n_nodes={n_nodes}\"\n",
    "full_directory = os.path.join(directory, folder_name)\n",
    "\n",
    "df_predictions = pd.read_pickle(os.path.join(full_directory, \"df_predictions.pkl\"))\n",
    "df_classifications = pd.read_pickle(os.path.join(full_directory, \"df_classifications.pkl\"))\n",
    "# df_accuracy = pd.read_csv(os.path.join(full_directory, \"df_accuracy.csv\"))\n",
    "\n",
    "\n",
    "\n",
    "# dict_df_metrics_SESI = {}\n",
    "# keys = [\"10\", \"20\", \"50\"]\t    \n",
    "# for key in keys:\n",
    "#     dict_df_metrics_SESI[key] = pd.read_csv(os.path.join(full_directory, f'Portfolio\\df_metrics_Equal_{key}_long_short.csv'))\n",
    "\n",
    "\n",
    "# weighting_method = \"Equal\"\t\n",
    "# dict_df_returns_portfolio = {}\n",
    "# for n_both in [10,20,50]:\n",
    "#     for str_side in [\"long_short\", \"long\", \"short\"]:\n",
    "#         key = f\"{weighting_method}_{n_both}_{str_side}\"\n",
    "#         dict_df_returns_portfolio[key] = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_{key}.pkl\"))\n",
    "\n",
    "\n",
    "# df_returns_portfolio_Equal_10_short = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_Equal_10_short.pkl\"))\n",
    "# df_returns_portfolio_Equal_10_long_short = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_Equal_10_long_short.pkl\"))\n",
    "# df_returns_portfolio_Equal_10_long = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_Equal_10_long.pkl\"))\n",
    "# df_returns_portfolio_Equal_20_short = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_Equal_20_short.pkl\"))\n",
    "# df_returns_portfolio_Equal_20_long_short = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_Equal_20_long_short.pkl\"))\n",
    "# df_returns_portfolio_Equal_20_long = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_Equal_20_long.pkl\"))\n",
    "# df_returns_portfolio_Equal_50_short = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_Equal_50_short.pkl\"))\n",
    "# df_returns_portfolio_Equal_50_long_short = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_Equal_50_long_short.pkl\"))\n",
    "# df_returns_portfolio_Equal_50_long = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_Equal_50_long.pkl\"))\n",
    "\n",
    "# df_returns_portfolio_Equal_10_short_SESI = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_Equal_10_short.pkl\"))\n",
    "# df_returns_portfolio_Equal_10_long_short_SESI = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_Equal_10_long_short.pkl\"))\n",
    "# df_returns_portfolio_Equal_10_long_SESI = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_Equal_10_long.pkl\"))\n",
    "# df_returns_portfolio_Equal_20_short_SESI = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_Equal_20_short.pkl\"))\n",
    "# df_returns_portfolio_Equal_20_long_short_SESI = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_Equal_20_long_short.pkl\"))\n",
    "# df_returns_portfolio_Equal_20_long_SESI = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_Equal_20_long.pkl\"))\n",
    "# df_returns_portfolio_Equal_50_short_SESI = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_Equal_50_short.pkl\"))\n",
    "# df_returns_portfolio_Equal_50_long_short_SESI = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_Equal_50_long_short.pkl\"))\n",
    "# df_returns_portfolio_Equal_50_long_SESI = pd.read_pickle(os.path.join(full_directory, f\"Portfolio\\df_returns_portfolio_Equal_50_long.pkl\"))\n",
    "\n",
    "\n",
    "\n",
    "# best_model_LSTM = load(os.path.join(full_directory, \"best_model_LSTM.joblib\"))\n",
    "# best_model_GBC = load(os.path.join(full_directory, \"best_model_GBC.joblib\"))\n",
    "# best_model_RF = load(os.path.join(full_directory, \"best_model_RF.joblib\"))\n",
    "# best_model_RNN = load(os.path.join(full_directory, \"best_model_RNN.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if b_SESI == True:\n",
    "    df_returns_portfolio = df_returns_portfolio_Equal_10_long_short_SESI.copy()\n",
    "else:\n",
    "    df_returns_portfolio = df_returns_portfolio_Equal_10_long_short.copy()\n",
    "\n",
    "if subsample == \"second_half\":\n",
    "    df_returns_portfolio = df_returns_portfolio[df_returns_portfolio[\"BarDate\"] > datetime.datetime.strptime(\"31/12/2019\", '%d/%m/%Y')]\n",
    "elif subsample == \"first_half\":\n",
    "    df_returns_portfolio = df_returns_portfolio[df_returns_portfolio[\"BarDate\"] <= datetime.datetime.strptime(\"01/01/2020\", '%d/%m/%Y')]      \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar charts for portfolio metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in dict_df_metrics:\n",
    "#     # set the index as column Unnamed: 0 and drop the column\n",
    "#     dict_df_metrics[key] = dict_df_metrics[key].set_index('Unnamed: 0')\n",
    "#     # Remove rows with index values information_ratio, appraisal_ratio, max_drawdown, max_1_day_loss, turnover\n",
    "#     dict_df_metrics[key] = dict_df_metrics[key].drop([\"information_ratio\", \"appraisal_ratio\", \"max_drawdown\", \"max_1_day_loss\", \"turnover\"])\n",
    "#     # Change column names to \"LR\", \"RF\", \"GB\", \"RNN\", \"LSTM\"\n",
    "#     dict_df_metrics[key].columns = [\"LR\", \"RF\", \"GB\", \"RNN\", \"LSTM\"]\n",
    "#     # Change index names to \"Return (%)\", 'St. Dev (%)', \"Sharpe Ratio\" \n",
    "#     dict_df_metrics[key].index = [\"Return (%)\", 'St. Dev. (%)', \"Sharpe Ratio\"]   \n",
    "    \n",
    "#     # multiply each value in the row \"Return\" and \"St. Dev.\" by 100\n",
    "#     dict_df_metrics[key].loc[\"Return (%)\"] = dict_df_metrics[key].loc[\"Return (%)\"] * 100\n",
    "#     dict_df_metrics[key].loc[\"St. Dev. (%)\"] = dict_df_metrics[key].loc[\"St. Dev. (%)\"] * 100\n",
    "    \n",
    "#     # # round the values in the dataframe to 3 decimals\n",
    "#     dict_df_metrics[key] = dict_df_metrics[key].round(3)\n",
    "\n",
    "for key in dict_df_metrics_SESI:\n",
    "    # set the index as column Unnamed: 0 and drop the column\n",
    "    dict_df_metrics_SESI[key] = dict_df_metrics_SESI[key].set_index('Unnamed: 0')\n",
    "    # Remove rows with index values information_ratio, appraisal_ratio, max_drawdown, max_1_day_loss, turnover\n",
    "    dict_df_metrics_SESI[key] = dict_df_metrics_SESI[key].drop([\"information_ratio\", \"appraisal_ratio\", \"max_drawdown\", \"max_1_day_loss\", \"turnover\"])\n",
    "    # Change column names to \"LR\", \"RF\", \"GB\", \"RNN\", \"LSTM\"\n",
    "    dict_df_metrics_SESI[key].columns = [\"LR\", \"RF\", \"GB\", \"RNN\", \"LSTM\"]\n",
    "    # Change index names to \"Return (%)\", 'St. Dev (%)', \"Sharpe Ratio\" \n",
    "    dict_df_metrics_SESI[key].index = [\"Return (%)\", 'St. Dev. (%)', \"Sharpe Ratio\"]   \n",
    "    \n",
    "    # multiply each value in the row \"Return\" and \"St. Dev.\" by 100\n",
    "    dict_df_metrics_SESI[key].loc[\"Return (%)\"] = dict_df_metrics_SESI[key].loc[\"Return (%)\"] * 100\n",
    "    dict_df_metrics_SESI[key].loc[\"St. Dev. (%)\"] = dict_df_metrics_SESI[key].loc[\"St. Dev. (%)\"] * 100\n",
    "    \n",
    "    # # round the values in the dataframe to 3 decimals\n",
    "    dict_df_metrics_SESI[key] = dict_df_metrics_SESI[key].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = dict_df_metrics_SESI\n",
    "\n",
    "# Define colors for each bar category\n",
    "colors = ['red', 'orange', 'yellow', 'green', 'blue']\n",
    "# colors = [(1, 0, 0, 1), (0, 0, 1, 1), (0, 1, 0, 1), (1, 0.5, 0, 1), (1, 1, 0, 1)]  # red, blue, green, orange, yellow # last number is the alpha transparency # Lower values for the first 3 lead to darker colors\n",
    "# colors = [(0.75, 0, 0, 1), (0, 0, 0.75, 1), (0, 0.75, 0, 1), (0.75, 0.375, 0, 1), (0.75, 0.75, 0, 1)] \n",
    "# colors = [(1.0, 0.0, 0.0), (0.75, 0.0, 0.25), (0.5, 0.0, 0.5), (0.25, 0.0, 0.75), (0.0, 0.0, 1.0)]     # from red to blue\n",
    "# # colors = [(0.0, 0.0, 1.0), (0.2, 0.2, 0.6), (0.4, 0.4, 0.2),(0.6, 0.6, 0.0),(1.0, 1.0, 0.0)]\n",
    "# # colors = [(0.0, 0.0, 1.0), (0.0, 0.25, 0.5), (0.0, 0.5, 0.0), (0.0, 0.75, 0.0), (0.0, 1.0, 0.0)]\n",
    "# colors = [(0.0, 0.0, 0), (0.0, 0.0, 0.25), (0.0, 0.0, 0.5), (0.0, 0.0, 0.75), (0.0, 0.0, 1.0)]        # Black to blue\n",
    "# #colors = [(1.0, 1.0, 1.0), (0.75, 0.75, 1.0), (0.5, 0.5, 1.0), (0.25, 0.25, 1.0), (0.0, 0.0, 1.0)]    # White to blue    \n",
    "# colors = [(0.0, 0.0, 0), (0.0, 0.0, 0.40), (0.0, 0.0, 0.80), (0.4, 0.4, 1), (0.8, 0.8, 1.0)]        # Black to light blue        # Best colours\n",
    "\n",
    "\n",
    "# Create subplots with 3 rows and 3 columns\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.1)\n",
    "\n",
    "metrics = [\"Return (%)\", 'St. Dev. (%)', \"Sharpe Ratio\"]\n",
    "keys = [\"10\", \"20\", \"50\"]\n",
    "\n",
    "# Define Y-axis limits for each metric\n",
    "y_lims = {\"Return (%)\": [0, 0.32], 'St. Dev. (%)': [0, 2.1], \"Sharpe Ratio\": [0, 3.9]}\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    for j, key in enumerate(keys):\n",
    "        bars = axes[i, j].bar(dict[key].columns, dict[key].loc[metric], color=colors)\n",
    "        if metric == \"Return (%)\":\n",
    "            axes[i, j].set_title(f\"K={key}\", fontsize = \"xx-large\")\n",
    "        # axes[i, j].set_xlabel(key)\n",
    "        if key == \"10\":\n",
    "            axes[i, j].set_ylabel(metric, fontsize = \"xx-large\")\n",
    "\n",
    "        axes[i, j].set_ylim(y_lims[metric])  # set the y-axis limit\n",
    "        axes[i, j].set_yticklabels([])\n",
    "\n",
    "        # Adjust the size of x and y tick labels\n",
    "        axes[i, j].tick_params(axis='x', labelsize=12)\n",
    "\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            # axes[i, j].text(bar.get_x() + bar.get_width() / 2, yval, round(yval, 3), ha='center', va='bottom', fontsize = \"large\")\n",
    "            axes[i, j].text(bar.get_x() + bar.get_width() / 2, yval, f\"{yval:.3f}\", ha='center', va='bottom', fontsize=\"x-large\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression on risk factors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Portfolio_backtester\n",
    "importlib.reload(Portfolio_backtester)\n",
    "\n",
    "# df_returns_portfolio_Equal_10_long_short and df_returns_portfolio_Equal_10_long_short_SESI needed\n",
    "\n",
    "str_returns_reg_models = [\"returns_lstm_portfolio\", \"returns_lr_portfolio_SESI\", \"returns_lstm_portfolio_SESI\"]\n",
    "dict_regression_results = Portfolio_backtester.regression(df_returns_portfolio_Equal_10_long_short, df_returns_portfolio_Equal_10_long_short_SESI, rf_rate_and_factors, str_returns_reg_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
